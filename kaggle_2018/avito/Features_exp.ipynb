{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import hstack, vstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re, pickle\n",
    "# from sklearn.svm import SVR\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "punct = set(punctuation)\n",
    "import scipy\n",
    "morph = MorphAnalyzer()\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    tokens = [x for x in tokenizer(text.lower()) if not set(x).issubset(punct)]\n",
    "    norm = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    return ' '.join(norm)\n",
    "\n",
    "def normalize_pos(text):\n",
    "    tokens = [x for x in tokenizer(text.lower()) if not set(x).issubset(punct)]\n",
    "    norm = filter(bool, [morph.parse(token)[0].tag.POS for token in tokens])\n",
    "    return ' '.join(norm)\n",
    "\n",
    "def sum_of_idfs(text, idfs):\n",
    "    tokens = [x for x in tokenizer(text.lower())]\n",
    "    return np.log10(sum([idfs.get(token, 8) for token in tokens])+1)\n",
    "\n",
    "def week(text):\n",
    "    y,m,d = [int(x) for x in text.split('-')]\n",
    "    date = datetime(y,m,d)\n",
    "    return date.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = train[['param_1', 'param_2', 'param_3']].fillna('nan').apply(lambda x: \n",
    "                                                        ' '.join([p for p in x]), axis=1)\n",
    "\n",
    "descriptions = train.description.fillna('nan')\n",
    "title = train.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_upper_descr = descriptions.apply(lambda x: len([j for j in x if j.isupper()])/len(x))\n",
    "per_digit_descr = descriptions.apply(lambda x: len([j for j in x if j.isdigit()])/len(x))\n",
    "per_excl_descr = descriptions.apply(lambda x: \n",
    "                                                len([j for j in x if j == '!'])/len(x))\n",
    "per_punct_descr = descriptions.apply(lambda x: \n",
    "                                                len([j for j in x if not j.isalnum() and j != ' '])/len(x))\n",
    "\n",
    "per_upper_title = title.apply(lambda x: len([j for j in x if j.isupper()])/len(x))\n",
    "per_digit_title = title.apply(lambda x: len([j for j in x if j.isdigit()])/len(x))\n",
    "per_punct_title = title.apply(lambda x: \n",
    "                                                len([j for j in x if not j.isalnum() and j != ' '])/len(x))\n",
    "per_excl_title = title.apply(lambda x: \n",
    "                                                len([j for j in x if j == '!'])/len(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mean_t = np.mean(train.title.apply(len))\n",
    "# mean_d = np.mean(train.description.fillna('').apply(len))\n",
    "# pd.concat([train.image_top_1.fillna(0), test.image_top_1.fillna(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_upper_descr_test = test.description.fillna(' ').apply(lambda x: len([j for j in x if j.isupper()])/len(x))\n",
    "per_digit_descr_test = test.description.fillna(' ').apply(lambda x: len([j for j in x if j.isdigit()])/len(x))\n",
    "per_excl_descr_test = test.description.fillna(' ').apply(lambda x: \n",
    "                                                len([j for j in x if j == '!'])/len(x))\n",
    "per_punct_descr_test = test.description.fillna(' ').apply(lambda x: \n",
    "                                                len([j for j in x if not j.isalnum() and j != ' '])/len(x))\n",
    "\n",
    "per_upper_title_test = test.title.fillna(' ').apply(lambda x: len([j for j in x if j.isupper()])/len(x))\n",
    "per_digit_title_test = test.title.fillna(' ').apply(lambda x: len([j for j in x if j.isdigit()])/len(x))\n",
    "per_punct_title_test = test.title.fillna(' ').apply(lambda x: \n",
    "                                                len([j for j in x if not j.isalnum() and j != ' '])/len(x))\n",
    "per_excl_title_test = test.title.fillna(' ').apply(lambda x: \n",
    "                                                len([j for j in x if j == '!'])/len(x))\n",
    "\n",
    "# mean_t = np.mean(train.title.apply(len))\n",
    "# mean_d = np.mean(train.description.fillna('').apply(len))\n",
    "# pd.concat([train.image_top_1.fillna(0), test.image_top_1.fillna(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_descr_train = descriptions.apply(lambda x: np.log10(len(x)+1)).values.reshape(-1,1)\n",
    "length_title_train = title.apply(lambda x: np.log10(len(x)+1)).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_descr_test = test.description.fillna(' ').apply(lambda x: np.log10(len(x)+1)).values.reshape(-1,1)\n",
    "length_title_test = test.title.fillna(' ').apply(lambda x: np.log10(len(x)+1)).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_descr_tokens_train = descriptions.apply(lambda x: np.log10(len(tokenizer(x))+1))\n",
    "length_title_tokens_train = title.apply(lambda x: np.log10(len(tokenizer(x))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_descr_tokens_test = descriptions_test.apply(lambda x: np.log10(len(tokenizer(x))+1))\n",
    "length_title_tokens_test = title_test.apply(lambda x: np.log10(len(tokenizer(x))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_descr_uniq_tokens_train = descriptions.apply(lambda x: np.log10(len(set(tokenizer(x)))+1))\n",
    "length_title_uniq_tokens_train = title.apply(lambda x: np.log10(len(set(tokenizer(x)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_descr_uniq_tokens_test = descriptions_test.apply(lambda x: np.log10(len(set(tokenizer(x)))+1))\n",
    "length_title_uniq_tokens_test = title_test.apply(lambda x: np.log10(len(set(tokenizer(x)))+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices_mentioned = train.description.fillna(' ').apply(find_prices).values.reshape(-1,1)\n",
    "# prices = train.price.fillna().apply(lambda x: np.log10(x+1)).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prices_mentioned_test = test.description.fillna(' ').apply(find_prices).values.reshape(-1,1)\n",
    "# prices_test = test.price.fillna(1300).apply(lambda x: np.log10(x+1)).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = train.price.apply(np.log10).fillna(0)\n",
    "prices_test = test.price.apply(np.log10).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_param_train = params.apply(len).apply(np.log10).fillna(0)\n",
    "length_param_test = params_test.apply(len).apply(np.log10).fillna(0)\n",
    "\n",
    "# length_param_test = test[['param_1', 'param_2', 'param_3']].fillna('').apply(lambda x: \n",
    "#                                                         ' '.join([p for p in x]), axis=1).apply(len).apply(np.log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_city_stat = np.load('matrix/X_city_stat.npy', )\n",
    "X_city_stat_price = np.load('matrix/X_city_stat_price.npy', )\n",
    "X_category_name_stat = np.load('matrix/X_category_name_stat.npy', )\n",
    "X_category_name_stat_price = np.load('matrix/X_category_name_stat_price.npy', )\n",
    "X_param_1_stat = np.load('matrix/X_param_1_stat.npy', )\n",
    "X_param_1_stat_price = np.load('matrix/X_param_1_stat_price.npy', )\n",
    "X_param_2_stat = np.load('matrix/X_param_2_stat.npy', )\n",
    "X_param_2_stat_price = np.load('matrix/X_param_2_stat_price.npy', )\n",
    "X_param_3_stat = np.load('matrix/X_param_3_stat.npy', )\n",
    "X_param_3_stat_price = np.load('matrix/X_param_3_stat_price.npy', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_city_stat_test = np.load('matrix/X_city_stat_test.npy', )\n",
    "X_city_stat_price_test = np.load('matrix/X_city_stat_price_test.npy', )\n",
    "X_category_name_stat_test = np.load('matrix/X_category_name_stat_test.npy', )\n",
    "X_category_name_stat_price_test = np.load('matrix/X_category_name_stat_price_test.npy', )\n",
    "X_param_1_stat_test = np.load('matrix/X_param_1_stat_test.npy', )\n",
    "X_param_1_stat_price_test = np.load('matrix/X_param_1_stat_price_test.npy', )\n",
    "X_param_2_stat_test = np.load('matrix/X_param_2_stat_test.npy', )\n",
    "X_param_2_stat_price_test = np.load('matrix/X_param_2_stat_price_test.npy', )\n",
    "X_param_3_stat_test = np.load('matrix/X_param_3_stat_test.npy', )\n",
    "X_param_3_stat_price_test = np.load('matrix/X_param_3_stat_price_test.npy', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ТОКЕНИЗАЦИЯ\n",
    "stops = stopwords.words('russian')\n",
    "rx = re.compile('\\d+\\,\\d+|[\\w]+(?:[\\w\\-/]+[\\w])?')\n",
    "rexc = re.compile('!!+')\n",
    "rtir = re.compile('--+')\n",
    "tokenizer = lambda x: [correct.get(m.lower().replace('ё','е'), \n",
    "                                  m.lower().replace('ё','е')) \n",
    "                       for string in re.sub('\\n', ' ', x).split() \n",
    "            for m in rx.findall(re.sub('[_\\xa0\\t]', ' ', string))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = {}\n",
    "for line in open('corrections.txt'):\n",
    "    line = line.rstrip('\\n')\n",
    "    mistake, cor = line.split('\\t')\n",
    "    correct[mistake] = cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПАРАМЕТРЫ, ОПИСАНИЕ, ЗАГОЛОВОК\n",
    "\n",
    "cv_params = CountVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "tf_descr = TfidfVectorizer(max_features=70000, ngram_range=(1,2),\n",
    "                           tokenizer=tokenizer, sublinear_tf=True, min_df=5)\n",
    "tf_title = TfidfVectorizer(max_features=20000, ngram_range=(1,2),\n",
    "                                 tokenizer=tokenizer, sublinear_tf=True, min_df=5, )\n",
    "# tf_descr.fit(descriptions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_params_train = cv_params.fit_transform(params)\n",
    "X_descr_train = tf_descr.fit_transform(descriptions)\n",
    "X_title_train = tf_title.fit_transform(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_title_char = TfidfVectorizer(max_features=2000, ngram_range=(1,3), \n",
    "                                analyzer='char', lowercase=True, min_df=500, \n",
    "                                sublinear_tf=False)\n",
    "X_title_char_train = tf_title_char.fit_transform(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_title_char_test = tf_title_char.transform(title_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs_descr = {k:v for k,v in zip(tf_descr.get_feature_names(), tf_descr.idf_)}\n",
    "idfs_title = {k:v for k,v in zip(tf_title.get_feature_names(), tf_title.idf_)}\n",
    "# idfs_param = {k:v for k,v in zip(cv_params.get_feature_names(), cv_params.idf_)}\n",
    "\n",
    "idfs_sum_title = descriptions.apply(lambda x: sum_of_idfs(x, idfs_title))\n",
    "idfs_sum_descr = title.fillna(' ').apply(lambda x: sum_of_idfs(x, idfs_descr))\n",
    "# idfs_sum_param = params.apply(lambda x: sum_of_idfs(x, idfs_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idfs_sum_title_test = descriptions_test.apply(lambda x: sum_of_idfs(x, idfs_title))\n",
    "idfs_sum_descr_test = title_test.fillna(' ').apply(lambda x: sum_of_idfs(x, idfs_descr))\n",
    "# idfs_sum_param = params.apply(lambda x: sum_of_idfs(x, idfs_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_params = params.apply(normalize_pos)\n",
    "pos_descriptions = descriptions.apply(normalize_pos)\n",
    "pos_title = title.apply(normalize_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_params_pos = TfidfVectorizer(min_df=100, ngram_range=(1,2))\n",
    "tf_title_pos = TfidfVectorizer(min_df=100, ngram_range=(1,2))\n",
    "tf_descr_pos = TfidfVectorizer(min_df=100, ngram_range=(1,2))\n",
    "\n",
    "X_pos_params_train = tf_params_pos.fit_transform(pos_params)\n",
    "X_pos_descriptions_train = tf_descr_pos.fit_transform(pos_descriptions)\n",
    "X_pos_title_train = tf_title_pos.fit_transform(pos_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_params = params.apply(normalize)\n",
    "norm_descriptions = descriptions.apply(normalize)\n",
    "norm_title = title.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_params_norm = TfidfVectorizer(min_df=100, ngram_range=(1,2))\n",
    "tf_title_norm = TfidfVectorizer(min_df=100, max_features=5000)\n",
    "tf_descr_norm = TfidfVectorizer(min_df=100, max_features=5000)\n",
    "\n",
    "# X_norm_params_train = tf_params_norm.fit_transform(pos_params)\n",
    "X_norm_descriptions_train = tf_descr_norm.fit_transform(norm_descriptions)\n",
    "X_norm_title_train = tf_title_norm.fit_transform(norm_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITEM SEQ NUMBER as NUMBER\n",
    "\n",
    "X_item_seq_number_n_train = train.item_seq_number.apply(np.log10).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITEM SEQ NUMBER as NUMBER\n",
    "\n",
    "X_item_seq_number_n_test = test.item_seq_number.apply(np.log10).values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = np.concatenate([length_descr_train,\n",
    "                        length_title_train,\n",
    "                        length_param_train.values.reshape(-1,1),\n",
    "                        prices.values.reshape(-1,1),\n",
    "                        length_descr_uniq_tokens_train.values.reshape(-1,1),\n",
    "                        length_title_uniq_tokens_train.values.reshape(-1,1),\n",
    "                        length_param_train.values.reshape(-1,1),\n",
    "                        per_digit_descr.values.reshape(-1,1),\n",
    "                        per_digit_title.values.reshape(-1,1),\n",
    "                        per_excl_descr.values.reshape(-1,1),\n",
    "                        per_excl_title.values.reshape(-1,1),\n",
    "                        per_punct_descr.values.reshape(-1,1),\n",
    "                        per_punct_title.values.reshape(-1,1),\n",
    "                        per_upper_descr.values.reshape(-1,1),\n",
    "                        per_upper_title.values.reshape(-1,1),\n",
    "                        X_item_seq_number_n_train,\n",
    "                        length_descr_tokens_train.values.reshape(-1,1),\n",
    "                        length_title_tokens_train.values.reshape(-1,1),\n",
    "                        idfs_sum_title.values.reshape(-1,1),\n",
    "                        idfs_sum_descr.values.reshape(-1,1),\n",
    "#                         X_city_stat.reshape(-1, 1),\n",
    "#                         X_city_stat_price.reshape(-1, 1),\n",
    "#                         X_category_name_stat.reshape(-1, 1),\n",
    "#                         X_category_name_stat_price.reshape(-1, 1),\n",
    "#                         X_param_1_stat.reshape(-1, 1),\n",
    "#                         X_param_1_stat_price.reshape(-1, 1),\n",
    "#                         X_param_2_stat.reshape(-1, 1),\n",
    "#                         X_param_2_stat_price.reshape(-1, 1),\n",
    "#                         X_param_3_stat.reshape(-1, 1),\n",
    "#                         X_param_3_stat_price.reshape(-1, 1)\n",
    "                        ], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_test = np.concatenate([length_descr_test,\n",
    "                        length_title_test,\n",
    "                        length_param_test.values.reshape(-1,1),\n",
    "                        prices_test.values.reshape(-1,1),\n",
    "                        length_descr_uniq_tokens_test.values.reshape(-1,1),\n",
    "                        length_title_uniq_tokens_test.values.reshape(-1,1),\n",
    "                        length_param_test.values.reshape(-1,1),\n",
    "                        per_digit_descr_test.values.reshape(-1,1),\n",
    "                        per_digit_title_test.values.reshape(-1,1),\n",
    "                        per_excl_descr_test.values.reshape(-1,1),\n",
    "                        per_excl_title_test.values.reshape(-1,1),\n",
    "                        per_punct_descr_test.values.reshape(-1,1),\n",
    "                        per_punct_title_test.values.reshape(-1,1),\n",
    "                        per_upper_descr_test.values.reshape(-1,1),\n",
    "                        per_upper_title_test.values.reshape(-1,1),\n",
    "                        X_item_seq_number_n_test,\n",
    "                        length_descr_tokens_test.values.reshape(-1,1),\n",
    "                        length_title_tokens_test.values.reshape(-1,1),\n",
    "                        idfs_sum_title_test.values.reshape(-1,1),\n",
    "                        idfs_sum_descr_test.values.reshape(-1,1),\n",
    "#                         X_city_stat_test.reshape(-1, 1),\n",
    "#                         X_city_stat_price_test.reshape(-1, 1),\n",
    "#                         X_category_name_stat_test.reshape(-1, 1),\n",
    "#                         X_category_name_stat_price_test.reshape(-1, 1),\n",
    "#                         X_param_1_stat_test.reshape(-1, 1),\n",
    "#                         X_param_1_stat_price_test.reshape(-1, 1),\n",
    "#                         X_param_2_stat_test.reshape(-1, 1),\n",
    "#                         X_param_2_stat_price_test.reshape(-1, 1),\n",
    "#                         X_param_3_stat_test.reshape(-1, 1),\n",
    "#                         X_param_3_stat_price_test.reshape(-1, 1)\n",
    "                        ], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pl = PolynomialFeatures(3, interaction_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[np.isinf(stats)] = -99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_test[np.isinf(stats_test)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_inter = pl.fit_transform(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1503424, 496)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОПИСАНИЕ, ПАРАМЕТРЫ, ЗАГОЛОВОК ДЛЯ ТЕСТА\n",
    "\n",
    "params_test = test[['param_1', 'param_2', 'param_3']].fillna('nan').apply(lambda x: \n",
    "                                                        ' '.join([p for p in x]), axis=1)\n",
    "\n",
    "descriptions_test = test.description.fillna('nan')\n",
    "title_test = test.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_params_test = cv_params.transform(params_test)\n",
    "\n",
    "X_descr_test = tf_descr.transform(descriptions_test)\n",
    "\n",
    "X_title_test = tf_title.transform(title_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS CAT\n",
    "\n",
    "lenc_param_1 = LabelEncoder()\n",
    "int_enc = lenc_param_1.fit_transform(pd.concat([train.param_1.fillna(''), \n",
    "                                        test.param_1.fillna('') ]))\n",
    "onehot_param_1 = OneHotEncoder(sparse=True)\n",
    "int_param_1 = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "onehot_param_1.fit(int_param_1)\n",
    "\n",
    "int_enc_train = lenc_param_1.transform(train.param_1.fillna(''))\n",
    "X_param_1_train = onehot_param_1.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "int_enc_test = lenc_param_1.transform(test.param_1.fillna(''))\n",
    "X_param_1_test = onehot_param_1.transform(int_enc_test.reshape(len(int_enc_test), 1))\n",
    "\n",
    "# PARAMS CAT\n",
    "\n",
    "lenc_param_2 = LabelEncoder()\n",
    "int_enc = lenc_param_2.fit_transform(pd.concat([train.param_2.fillna(''), \n",
    "                                        test.param_2.fillna('') ]))\n",
    "onehot_param_2 = OneHotEncoder(sparse=True)\n",
    "int_param_2 = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "onehot_param_2.fit(int_param_2)\n",
    "\n",
    "int_enc_train = lenc_param_2.transform(train.param_2.fillna(''))\n",
    "X_param_2_train = onehot_param_2.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "int_enc_test = lenc_param_2.transform(test.param_2.fillna(''))\n",
    "X_param_2_test = onehot_param_2.transform(int_enc_test.reshape(len(int_enc_test), 1))\n",
    "\n",
    "# PARAMS CAT\n",
    "\n",
    "lenc_param_3 = LabelEncoder()\n",
    "int_enc = lenc_param_3.fit_transform(pd.concat([train.param_3.fillna(''), \n",
    "                                        test.param_3.fillna('') ]))\n",
    "onehot_param_3 = OneHotEncoder(sparse=True)\n",
    "int_param_3 = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "onehot_param_3.fit(int_param_3)\n",
    "\n",
    "int_enc_train = lenc_param_3.transform(train.param_3.fillna(''))\n",
    "X_param_3_train = onehot_param_3.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "int_enc_test = lenc_param_3.transform(test.param_3.fillna(''))\n",
    "X_param_3_test = onehot_param_3.transform(int_enc_test.reshape(len(int_enc_test), 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IMAGE TOP 1\n",
    "\n",
    "lenc = LabelEncoder()\n",
    "int_enc = lenc.fit_transform(pd.concat([train.image_top_1.fillna(0), \n",
    "                                        test.image_top_1.fillna(0)]))\n",
    "onehot = OneHotEncoder(sparse=True)\n",
    "int_top1 = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "onehot.fit(int_top1)\n",
    "\n",
    "int_enc_train = lenc.transform(train.image_top_1.fillna(0))\n",
    "X_top1_train = onehot.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "int_enc_test = lenc.transform(test.image_top_1.fillna(0))\n",
    "X_top1_test = onehot.transform(int_enc_test.reshape(len(int_enc_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lenc_user_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-3191411d036a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlenc_user_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lenc_user_id' is not defined"
     ]
    }
   ],
   "source": [
    "lenc_user_id.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_set = set(train.user_id) & set(test.user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67929"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = Counter(test.user_id) + Counter(train.user_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(lenc_user_id.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shrink(userid):\n",
    "#     if userid in vocab:\n",
    "        \n",
    "user_id = train.user_id.apply(lambda x: str(freqs.get(x, 'shit')) if x not in ids_set else x)\n",
    "user_id_test = test.user_id.apply(lambda x: str(freqs.get(x, 'shit')) if x not in ids_set else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = pd.concat([user_id, user_id_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER ID\n",
    "\n",
    "lenc_user_id = LabelEncoder()\n",
    "int_enc = lenc_user_id.fit_transform(conc)\n",
    "onehot_user_id = OneHotEncoder(sparse=True)\n",
    "int_user_id = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "onehot_user_id.fit(int_user_id)\n",
    "\n",
    "int_enc_train = lenc_user_id.transform(user_id)\n",
    "X_user_id_train = onehot_user_id.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "\n",
    "int_enc_test = lenc_user_id.transform(user_id_test)\n",
    "X_user_id_test = onehot_user_id.transform(int_enc_test.reshape(len(int_enc_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_user_id_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITEM SEQ NUMBER\n",
    "\n",
    "# lenc_item_seq_number = LabelEncoder()\n",
    "# int_enc = lenc_item_seq_number.fit_transform(pd.concat([train.item_seq_number.fillna(0), \n",
    "#                                                test.item_seq_number.fillna(0)]))\n",
    "# onehot_item_seq_number = OneHotEncoder(sparse=True)\n",
    "# int_item_seq_number = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "# onehot_item_seq_number.fit(int_item_seq_number)\n",
    "\n",
    "# int_enc_train = lenc_item_seq_number.transform(train.item_seq_number.fillna(0))\n",
    "# X_item_seq_number_train = onehot_item_seq_number.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "int_enc_test = lenc_item_seq_number.transform(test.item_seq_number.fillna(0))\n",
    "X_item_seq_number_test = onehot_item_seq_number.transform(int_enc_test.reshape(len(int_enc_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USER TYPE\n",
    "lenc_user_type = LabelEncoder()\n",
    "int_enc = lenc_user_type.fit_transform(pd.concat([train.user_type.fillna(0), \n",
    "                                               test.user_type.fillna(0)]))\n",
    "onehot_user_type = OneHotEncoder(sparse=True)\n",
    "int_user_type = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "onehot_user_type.fit(int_user_type)\n",
    "\n",
    "int_enc_train = lenc_user_type.transform(train.user_type.fillna(0))\n",
    "X_user_type_train = onehot_user_type.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "int_enc_test = lenc_user_type.transform(test.user_type.fillna(0))\n",
    "X_user_type_test = onehot_user_type.transform(int_enc_test.reshape(len(int_enc_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORY NAME\n",
    "\n",
    "lenc_category_name = LabelEncoder()\n",
    "int_enc = lenc_category_name.fit_transform(pd.concat([train.category_name.fillna(0), \n",
    "                                               test.category_name.fillna(0)]))\n",
    "onehot_category_name = OneHotEncoder(sparse=True)\n",
    "int_category_name = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "onehot_category_name.fit(int_category_name)\n",
    "\n",
    "int_enc_train = lenc_category_name.transform(train.category_name.fillna(0))\n",
    "X_category_name_train = onehot_category_name.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "int_enc_test = lenc_category_name.transform(test.category_name.fillna(0))\n",
    "X_category_name_test = onehot_category_name.transform(int_enc_test.reshape(len(int_enc_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARENT CATEGORY NAME\n",
    "\n",
    "lenc_parent_category_name = LabelEncoder()\n",
    "int_enc = lenc_parent_category_name.fit_transform(pd.concat([train.parent_category_name.fillna(0), \n",
    "                                               test.parent_category_name.fillna(0)]))\n",
    "onehot_parent_category_name = OneHotEncoder(sparse=True)\n",
    "int_parent_category_name = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "onehot_parent_category_name.fit(int_parent_category_name)\n",
    "\n",
    "int_enc_train = lenc_parent_category_name.transform(train.parent_category_name.fillna(0))\n",
    "X_parent_category_name_train = onehot_parent_category_name.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "int_enc_test = lenc_parent_category_name.transform(test.parent_category_name.fillna(0))\n",
    "X_parent_category_name_test = onehot_parent_category_name.transform(int_enc_test.reshape(len(int_enc_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITY\n",
    "\n",
    "lenc_city = LabelEncoder()\n",
    "int_enc = lenc_city.fit_transform(pd.concat([train.city, \n",
    "                                               test.city]))\n",
    "onehot_city = OneHotEncoder(sparse=True)\n",
    "int_city = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "onehot_city.fit(int_city)\n",
    "\n",
    "int_enc_train = lenc_city.transform(train.city)\n",
    "X_city_train = onehot_city.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "int_enc_test = lenc_city.transform(test.city)\n",
    "X_city_test = onehot_city.transform(int_enc_test.reshape(len(int_enc_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGION\n",
    "\n",
    "lenc_region = LabelEncoder()\n",
    "int_enc = lenc_region.fit_transform(pd.concat([train.region, \n",
    "                                               test.region]))\n",
    "onehot_region = OneHotEncoder(sparse=True)\n",
    "int_region = int_enc.reshape(len(int_enc), 1)\n",
    "\n",
    "onehot_region.fit(int_region)\n",
    "\n",
    "int_enc_train = lenc_region.transform(train.region)\n",
    "X_region_train = onehot_region.transform(int_enc_train.reshape(len(int_enc_train), 1))\n",
    "\n",
    "int_enc_test = lenc_region.transform(test.region)\n",
    "X_region_test = onehot_region.transform(int_enc_test.reshape(len(int_enc_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fasttext cluster\n",
    "\n",
    "ft_cluster_train = pickle.load(open('ft_labels.pkl', 'rb'))\n",
    "ft_cluster_test = pickle.load(open('ft_labels_test.pkl', 'rb'))\n",
    "\n",
    "onehot_ft_cluster = OneHotEncoder(sparse=True)\n",
    "int_ft_cluster_train = ft_cluster_train.reshape(len(ft_cluster_train), 1)\n",
    "int_ft_cluster_test = ft_cluster_test.reshape(len(ft_cluster_test), 1)\n",
    "# int_week_test = week_test.reshape(len(week_test), 1)\n",
    "\n",
    "X_ft_cluster_train = onehot_ft_cluster.fit_transform(int_ft_cluster_train)\n",
    "X_ft_cluster_test = onehot_ft_cluster.transform(int_ft_cluster_test)\n",
    "# X_week_test = onehot_week.transform(int_week_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEEK\n",
    "\n",
    "week_train = train.activation_date.apply(week)\n",
    "week_test = test.activation_date.apply(week)\n",
    "\n",
    "onehot_week = OneHotEncoder(sparse=True)\n",
    "int_week_train = week_train.reshape(len(week_train), 1)\n",
    "int_week_test = week_test.reshape(len(week_test), 1)\n",
    "\n",
    "X_week_train = onehot_week.fit_transform(int_week_train)\n",
    "X_week_test = onehot_week.transform(int_week_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "has_price_train = train.price.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)\n",
    "has_img_train = train.image.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)\n",
    "\n",
    "has_price_test = test.price.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)\n",
    "has_img_test = test.image.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_descr_train = train.description.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)\n",
    "has_param_1_train = train.param_1.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)\n",
    "\n",
    "has_param_2_train = train.param_2.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)\n",
    "has_param_3_train = train.param_3.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_descr_test = test.description.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)\n",
    "has_param_1_test = test.param_1.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)\n",
    "\n",
    "has_param_2_test = test.param_2.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)\n",
    "has_param_3_test = test.param_3.isna().apply(lambda x: 0 if x else 1).values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОБУЧАЮЩАЯ ВЫБОРКА\n",
    "\n",
    "X = hstack([X_params_train, X_descr_train, X_title_train,\n",
    "#             X_norm_descriptions_train, X_norm_title_train,\n",
    "#             X_pos_descriptions_train, X_pos_title_train, \n",
    "#             X_pos_params_train,\n",
    "            X_user_type_train, X_user_id_train, X_top1_train,\n",
    "            X_category_name_train, X_parent_category_name_train,\n",
    "            X_city_train, X_region_train,\n",
    "            X_week_train, has_price_train, has_img_train, has_descr_train,\n",
    "            has_param_1_train,\n",
    "            has_param_2_train,\n",
    "            has_param_3_train,\n",
    "            X_ft_cluster_train,\n",
    "            X_param_1_train,\n",
    "            X_param_2_train,\n",
    "            X_param_3_train,\n",
    "            X_title_char_train,\n",
    "            X_item_seq_number_train,\n",
    "#             X_city_stat.reshape(-1,1),\n",
    "#             X_city_stat_price.reshape(-1,1),\n",
    "#             X_category_name_stat.reshape(-1,1),\n",
    "#             X_category_name_stat_price.reshape(-1,1),\n",
    "#             X_param_1_stat.reshape(-1,1),\n",
    "#             X_param_1_stat_price.reshape(-1,1),\n",
    "#             X_param_2_stat.reshape(-1,1),\n",
    "#             X_param_2_stat_price.reshape(-1,1),\n",
    "#             X_param_3_stat.reshape(-1,1),\n",
    "#             X_param_3_stat_price.reshape(-1,1),\n",
    "#             X_params_char_train,\n",
    "#             X_title_char_train,\n",
    "            stats\n",
    "#             X_descr_char_train\n",
    "#             np.log10(user_mean_delay+1),\n",
    "#             np.log10(user_placed_ads+1),\n",
    "            \n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОБУЧАЮЩАЯ ВЫБОРКА\n",
    "\n",
    "X_test = hstack([X_params_test, X_descr_test, X_title_test,\n",
    "#             X_norm_descriptions_train, X_norm_title_train,\n",
    "#             X_pos_descriptions_train, X_pos_title_train, \n",
    "#             X_pos_params_train,\n",
    "            X_user_type_test, X_user_id_test, X_top1_test,\n",
    "            X_category_name_test, X_parent_category_name_test,\n",
    "            X_city_test, X_region_test,\n",
    "            X_week_test, has_price_test, has_img_test, has_descr_test,\n",
    "            has_param_1_test,\n",
    "            has_param_2_test,\n",
    "            has_param_3_test,\n",
    "            X_ft_cluster_test,\n",
    "            X_param_1_test,\n",
    "            X_param_2_test,\n",
    "            X_param_3_test,\n",
    "             X_title_char_test,\n",
    "            X_item_seq_number_test,\n",
    "#             X_city_stat.reshape(-1,1),\n",
    "#             X_city_stat_price.reshape(-1,1),\n",
    "#             X_category_name_stat.reshape(-1,1),\n",
    "#             X_category_name_stat_price.reshape(-1,1),\n",
    "#             X_param_1_stat.reshape(-1,1),\n",
    "#             X_param_1_stat_price.reshape(-1,1),\n",
    "#             X_param_2_stat.reshape(-1,1),\n",
    "#             X_param_2_stat_price.reshape(-1,1),\n",
    "#             X_param_3_stat.reshape(-1,1),\n",
    "#             X_param_3_stat_price.reshape(-1,1),\n",
    "#             X_params_char_train,\n",
    "#             X_title_char_train,\n",
    "            stats_test\n",
    "#             X_descr_char_train\n",
    "#             np.log10(user_mean_delay+1),\n",
    "#             np.log10(user_placed_ads+1),\n",
    "            \n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.deal_probability.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-3db099495351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1503424, 207827)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=10, copy_X=True, fit_intercept=False, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='sag', tol=0.001)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = Ridge(alpha=10, solver='sag', fit_intercept=False)\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2251409026823776\n"
     ]
    }
   ],
   "source": [
    "y_pred = regr.predict(X_valid)\n",
    "y_preds = []\n",
    "for j in y_pred:\n",
    "    if j < 0:\n",
    "        j = 0\n",
    "    elif j > 1:\n",
    "        j = 0.9\n",
    "    y_preds.append(j)\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_valid, y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBRegressor  \n",
    "import xgboost as xgb\n",
    "import scipy.stats as st\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X, label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {  \n",
    "    \"n_estimators\": 10000,\n",
    "    \"max_depth\": 10,\n",
    "    \"learning_rate\": 0.3,\n",
    "    \"nthreads\":3,\n",
    "    \"lambda\":10,\n",
    "    \"colsample_bytree\":0.9,\n",
    "    \"subsample\":0.9,\n",
    "    \"objective\":'reg:linear',\n",
    "#     'booster':'gblinear'\n",
    "#     \"colsample_bytree\": one_to_left,\n",
    "#     \"subsample\": one_to_left,\n",
    "#     \"gamma\": st.uniform(0, 10),\n",
    "#     'reg_alpha': from_zero_positive,\n",
    "#     \"min_child_weight\": from_zero_positive,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "evallist = [(dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.353175\n",
      "[1]\ttrain-rmse:0.297997\n",
      "[2]\ttrain-rmse:0.265929\n",
      "[3]\ttrain-rmse:0.248351\n",
      "[4]\ttrain-rmse:0.238402\n",
      "[5]\ttrain-rmse:0.23329\n",
      "[6]\ttrain-rmse:0.230112\n",
      "[7]\ttrain-rmse:0.228307\n",
      "[8]\ttrain-rmse:0.226885\n",
      "[9]\ttrain-rmse:0.226282\n",
      "[10]\ttrain-rmse:0.225757\n",
      "[11]\ttrain-rmse:0.225372\n",
      "[12]\ttrain-rmse:0.224937\n",
      "[13]\ttrain-rmse:0.22436\n",
      "[14]\ttrain-rmse:0.224185\n",
      "[15]\ttrain-rmse:0.223909\n",
      "[16]\ttrain-rmse:0.223646\n",
      "[17]\ttrain-rmse:0.223451\n",
      "[18]\ttrain-rmse:0.223416\n",
      "[19]\ttrain-rmse:0.223224\n",
      "[20]\ttrain-rmse:0.223224\n",
      "[21]\ttrain-rmse:0.223031\n",
      "[22]\ttrain-rmse:0.222618\n",
      "[23]\ttrain-rmse:0.222618\n",
      "[24]\ttrain-rmse:0.222618\n",
      "[25]\ttrain-rmse:0.222565\n",
      "[26]\ttrain-rmse:0.222565\n",
      "[27]\ttrain-rmse:0.222565\n",
      "[28]\ttrain-rmse:0.222404\n",
      "[29]\ttrain-rmse:0.222404\n",
      "[30]\ttrain-rmse:0.222404\n",
      "[31]\ttrain-rmse:0.221899\n",
      "[32]\ttrain-rmse:0.221899\n",
      "[33]\ttrain-rmse:0.221899\n",
      "[34]\ttrain-rmse:0.221899\n",
      "[35]\ttrain-rmse:0.221831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-adaca0f716de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevallist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.72-py3.5.egg/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.72-py3.5.egg/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.72-py3.5.egg/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 897\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    898\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_round = 100\n",
    "bst = xgb.train(params, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.save_model('xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.Booster({'nthread': 4})  # init model\n",
    "bst.load_model('xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.218404\n",
      "[1]\ttrain-rmse:0.218404\n",
      "[2]\ttrain-rmse:0.218349\n",
      "[3]\ttrain-rmse:0.218349\n",
      "[4]\ttrain-rmse:0.218298\n",
      "[5]\ttrain-rmse:0.218298\n",
      "[6]\ttrain-rmse:0.218295\n",
      "[7]\ttrain-rmse:0.218258\n",
      "[8]\ttrain-rmse:0.218258\n",
      "[9]\ttrain-rmse:0.218258\n",
      "[10]\ttrain-rmse:0.218258\n",
      "[11]\ttrain-rmse:0.218224\n",
      "[12]\ttrain-rmse:0.218215\n",
      "[13]\ttrain-rmse:0.218215\n",
      "[14]\ttrain-rmse:0.218215\n",
      "[15]\ttrain-rmse:0.218215\n",
      "[16]\ttrain-rmse:0.218176\n",
      "[17]\ttrain-rmse:0.218134\n",
      "[18]\ttrain-rmse:0.218075\n",
      "[19]\ttrain-rmse:0.218037\n",
      "[20]\ttrain-rmse:0.218037\n",
      "[21]\ttrain-rmse:0.21803\n",
      "[22]\ttrain-rmse:0.218027\n",
      "[23]\ttrain-rmse:0.217984\n",
      "[24]\ttrain-rmse:0.217984\n",
      "[25]\ttrain-rmse:0.217984\n",
      "[26]\ttrain-rmse:0.217984\n",
      "[27]\ttrain-rmse:0.217984\n",
      "[28]\ttrain-rmse:0.217866\n",
      "[29]\ttrain-rmse:0.217866\n",
      "[30]\ttrain-rmse:0.217866\n",
      "[31]\ttrain-rmse:0.217795\n",
      "[32]\ttrain-rmse:0.217747\n",
      "[33]\ttrain-rmse:0.217692\n",
      "[34]\ttrain-rmse:0.217682\n",
      "[35]\ttrain-rmse:0.21765\n",
      "[36]\ttrain-rmse:0.217638\n",
      "[37]\ttrain-rmse:0.217638\n",
      "[38]\ttrain-rmse:0.217605\n",
      "[39]\ttrain-rmse:0.2176\n",
      "[40]\ttrain-rmse:0.2176\n",
      "[41]\ttrain-rmse:0.2176\n",
      "[42]\ttrain-rmse:0.2176\n",
      "[43]\ttrain-rmse:0.2176\n",
      "[44]\ttrain-rmse:0.217568\n",
      "[45]\ttrain-rmse:0.217568\n",
      "[46]\ttrain-rmse:0.217568\n",
      "[47]\ttrain-rmse:0.217556\n",
      "[48]\ttrain-rmse:0.217556\n",
      "[49]\ttrain-rmse:0.217515\n",
      "[50]\ttrain-rmse:0.217515\n",
      "[51]\ttrain-rmse:0.217515\n",
      "[52]\ttrain-rmse:0.217515\n",
      "[53]\ttrain-rmse:0.217461\n",
      "[54]\ttrain-rmse:0.217447\n",
      "[55]\ttrain-rmse:0.217382\n",
      "[56]\ttrain-rmse:0.217368\n",
      "[57]\ttrain-rmse:0.217368\n",
      "[58]\ttrain-rmse:0.217368\n",
      "[59]\ttrain-rmse:0.217323\n",
      "[60]\ttrain-rmse:0.217323\n",
      "[61]\ttrain-rmse:0.217323\n",
      "[62]\ttrain-rmse:0.217323\n",
      "[63]\ttrain-rmse:0.217323\n",
      "[64]\ttrain-rmse:0.217323\n",
      "[65]\ttrain-rmse:0.217307\n",
      "[66]\ttrain-rmse:0.217307\n",
      "[67]\ttrain-rmse:0.217307\n",
      "[68]\ttrain-rmse:0.217307\n",
      "[69]\ttrain-rmse:0.217298\n",
      "[70]\ttrain-rmse:0.217298\n",
      "[71]\ttrain-rmse:0.217298\n",
      "[72]\ttrain-rmse:0.217298\n",
      "[73]\ttrain-rmse:0.217298\n",
      "[74]\ttrain-rmse:0.217298\n",
      "[75]\ttrain-rmse:0.21727\n",
      "[76]\ttrain-rmse:0.217221\n",
      "[77]\ttrain-rmse:0.217221\n",
      "[78]\ttrain-rmse:0.217193\n",
      "[79]\ttrain-rmse:0.217193\n",
      "[80]\ttrain-rmse:0.217193\n",
      "[81]\ttrain-rmse:0.217193\n",
      "[82]\ttrain-rmse:0.217193\n",
      "[83]\ttrain-rmse:0.217042\n",
      "[84]\ttrain-rmse:0.217042\n",
      "[85]\ttrain-rmse:0.217002\n",
      "[86]\ttrain-rmse:0.217002\n",
      "[87]\ttrain-rmse:0.216967\n",
      "[88]\ttrain-rmse:0.216967\n",
      "[89]\ttrain-rmse:0.216967\n",
      "[90]\ttrain-rmse:0.216956\n",
      "[91]\ttrain-rmse:0.216956\n",
      "[92]\ttrain-rmse:0.216933\n",
      "[93]\ttrain-rmse:0.216896\n",
      "[94]\ttrain-rmse:0.216896\n",
      "[95]\ttrain-rmse:0.216896\n",
      "[96]\ttrain-rmse:0.216861\n",
      "[97]\ttrain-rmse:0.216826\n",
      "[98]\ttrain-rmse:0.216794\n",
      "[99]\ttrain-rmse:0.216736\n"
     ]
    }
   ],
   "source": [
    "num_round = 100\n",
    "bst = xgb.train(params, dtrain, num_round, evallist,  xgb_model=bst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17934903, 0.22425888, 0.1936817 , ..., 0.04652682, 0.35061284,\n",
       "       0.10118181])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('submission_41.txt', 'w')\n",
    "f.write('item_id,deal_probability\\n')\n",
    "for i, proba in list(zip(test.item_id.values, y_pred)):\n",
    "    if proba < 0:\n",
    "        proba = 0\n",
    "    elif proba > 1:\n",
    "        proba = 0.9\n",
    "    f.write(','.join([i, str(proba)]) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['6544e41a8817', '65b9484d670f', '8bab230b2ecd', ...,\n",
       "       'a22a2eeb5dd2', 'ed7fbb0733c1', 'd374d332992f'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.item_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14005741, 0.2369861 , 0.18089867, ..., 0.03736338, 0.33610606,\n",
       "       0.11328503], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13847348, 0.24325353, 0.19394764, ..., 0.03695092, 0.33129305,\n",
       "       0.11287257], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
