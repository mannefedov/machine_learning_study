{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_20_cv_splits(PATH_TO_DATA):\n",
    "    #stratify_classes = y\n",
    "    train = pd.read_csv(os.path.join(PATH_TO_DATA, 'train.csv'), usecols=['target'])\n",
    "    stratify_classes =  train.target.apply(lambda x: int(np.log10(x)))\n",
    "    splits = {}\n",
    "    for random_state in range(20):\n",
    "        column = np.zeros(train.shape[0])\n",
    "        sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=random_state)\n",
    "        for i, (_, test_index) in enumerate(sss.split(train, stratify_classes)):\n",
    "            column[test_index] = i\n",
    "\n",
    "        splits[\"split{}\".format(random_state)] = column\n",
    "\n",
    "    pd.DataFrame(splits, index=train.index).to_csv(os.path.join(PATH_TO_DATA, 'folds/cv_splits_cleandata_stat_bin_red.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # function to generate 100 folds from create_folds_from_cv_splits func\n",
    "def create_folds_from_cv_splits(in_path):\n",
    "    \n",
    "    cv_splits = pd.read_csv(os.path.join(PATH_TO_DATA, in_path))\n",
    "    folds_list = []\n",
    "    for ind, i in enumerate(cv_splits.columns[1:]):\n",
    "        folds = list(set(cv_splits[i].values))\n",
    "        folds_list.append([])\n",
    "        for m in folds:\n",
    "            val_idx = list(cv_splits[cv_splits[i]==m].index)\n",
    "            train_idx = list(set(list(cv_splits.index)) - set(val_idx))\n",
    "            folds_list[ind].append((train_idx, val_idx))\n",
    "    with open(os.path.join(PATH_TO_DATA, 'folds/custom_cv.pkl'), 'wb') as f:\n",
    "        pickle.dump(folds_list, f)\n",
    "    return folds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_CV = False\n",
    "\n",
    "if LOAD_CV:\n",
    "    with open(os.path.join(PATH_TO_DATA, 'folds/custom_cv.pkl'), 'rb') as f:\n",
    "        cv_folds = pickle.load(f)\n",
    "else:\n",
    "    get_20_cv_splits(PATH_TO_DATA)\n",
    "    cv_folds = create_folds_from_cv_splits(in_path='folds/cv_splits_cleandata_stat_bin_red.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb(train_X, train_y, val_X, val_y, test_X):\n",
    "    params = {'n_estimators':200,\n",
    "             'max_depth':4, \n",
    "             'lambda':10000,\n",
    "             'eta': 0.3, \n",
    "            \"colsample_bytree\":0.8,\n",
    "    #          \"tweedie_variance_power\":1.5,\n",
    "             'objective': 'reg:linear', \n",
    "             'eval_metric':'rmse',\n",
    "            'gamma':0.1}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    xgb_train = xgb.DMatrix(train_X, label=train_y)\n",
    "    xgb_val = xgb.DMatrix(val_X, label=val_y)\n",
    "    model = xgb.train(params, xgb_train, 200, \n",
    "                      evals=[(xgb_train, 'train'), (xgb_val, 'val')], \n",
    "                      early_stopping_rounds=30, \n",
    "                      verbose_eval=30)\n",
    "    print('Model training done in {} seconds.'.format(time.time() - start_time))\n",
    "    \n",
    "    pred_test_y = np.expm1(model.predict(test_X, ntree_limit=model.best_iteration))\n",
    "    pred_oof_log = model.predict(val_X, ntree_limit=model.best_iteration)\n",
    "    return pred_test_y, pred_oof_log, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_xgb(train_X, train_y, val_X, val_y, test_X, params):\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    start_time = time.time()\n",
    "    xgb_train = xgb.DMatrix(train_X, train_y)\n",
    "    xgb_val = xgb.DMatrix(val_X, val_y)\n",
    "    model = xgb.train(params, xgb_train, 1200, \n",
    "                      evals=[(xgb_train, 'train'), (xgb_val, 'val')], \n",
    "                      early_stopping_rounds=30, \n",
    "                      verbose_eval=30)\n",
    "    print('Model training done in {} seconds.'.format(time.time() - start_time))\n",
    "    xgb_test = xgb.DMatrix(test_X)\n",
    "    pred_test_y = np.expm1(model.predict(xgb_test, ntree_limit=model.best_iteration))\n",
    "    pred_oof_log = model.predict(xgb_val, ntree_limit=model.best_iteration)\n",
    "    return pred_test_y, pred_oof_log, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = pickle.load(open('features_groups.pkl', 'rb'))\n",
    "\n",
    "# def preprocess(train_X, val_X):\n",
    "#     train_X_stats = pd.DataFrame(index=train_X.index)\n",
    "#     val_X_stats = pd.DataFrame(index=val_X.index)\n",
    "    \n",
    "    \n",
    "#     for j, cols in enumerate([cols0] + feature_groups[:10]):\n",
    "#         train_X_stats['ts_last_mean_{}'.format(j)] = train_X[cols].apply(np.mean, axis=1)\n",
    "#         train_X_stats['ts_last_median_{}'.format(j)] = train_X[cols].apply(np.median, axis=1)\n",
    "#         train_X_stats['ts_last_sum_{}'.format(j)] = train_X[cols].apply(np.sum, axis=1)\n",
    "#         train_X_stats['ts_last_nonzero_{}'.format(j)] = train_X[cols].apply(lambda x: np.mean([1 if v > 0 else 0 for v in x]), axis=1)\n",
    "#         train_X_stats['ts_last_nonzero_mean_{}'.format(j)] = train_X[cols].apply(lambda x: np.mean([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "#         train_X_stats['ts_last_nonzero_median_{}'.format(j)] = train_X[cols].apply(lambda x: np.median([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "        \n",
    "#         val_X_stats['ts_last_mean_{}'.format(j)] = val_X[cols].apply(np.mean, axis=1)\n",
    "#         val_X_stats['ts_last_median_{}'.format(j)] = val_X[cols].apply(np.median, axis=1)\n",
    "#         val_X_stats['ts_last_sum_{}'.format(j)] = val_X[cols].apply(np.sum, axis=1)\n",
    "#         val_X_stats['ts_last_nonzero_{}'.format(j)] = val_X[cols].apply(lambda x: np.mean([1 if v > 0 else 0 for v in x]), axis=1)\n",
    "#         val_X_stats['ts_last_nonzero_mean_{}'.format(j)] = val_X[cols].apply(lambda x: np.mean([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "#         val_X_stats['ts_last_nonzero_median_{}'.format(j)] = val_X[cols].apply(lambda x: np.median([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "        \n",
    "\n",
    "#     return train_X_stats, val_X_stats\n",
    "def garb_out(x):\n",
    "    c = Counter(x)\n",
    "    clean = [j if c[j] <= 2 else 0 for j in x]\n",
    "    return clean\n",
    "\n",
    "def preprocess(test_X):\n",
    "    test_X_stats = pd.DataFrame(index=test_X.index)\n",
    "    \n",
    "    for j, cols in enumerate([cols0] + feature_groups):\n",
    "        test_X_col = test_X[cols].apply(garb_out, axis=1)\n",
    "        test_X_stats['ts_last_mean_{}'.format(j)] = test_X_col.apply(np.mean, axis=1)\n",
    "        test_X_stats['ts_last_median_{}'.format(j)] = test_X_col.apply(np.median, axis=1)\n",
    "        test_X_stats['ts_last_sum_{}'.format(j)] = test_X_col.apply(np.sum, axis=1)\n",
    "        test_X_stats['ts_last_nonzero_{}'.format(j)] = test_X_col.apply(lambda x: np.mean([1 if v > 0 else 0 for v in x]), axis=1)\n",
    "        test_X_stats['ts_last_nonzero_mean_{}'.format(j)] = test_X_col.apply(lambda x: np.mean([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "        test_X_stats['ts_last_nonzero_max_{}'.format(j)] = test_X_col.apply(lambda x: np.max([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "        test_X_stats['ts_last_nonzero_min_{}'.format(j)] = test_X_col.apply(lambda x: np.min([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "        test_X_stats['ts_last_nonzero_median_{}'.format(j)] = test_X_col.apply(lambda x: np.median([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "        test_X_stats['slope'] = test_X_col.apply(lambda x: linregress(list(range(len(cols))), x.values)[0], axis=1)\n",
    "    return test_X_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_calculations(X, test, big_cv_folds, func_name, params):\n",
    "    if not func_name:\n",
    "        return print('The function to run is not defined')\n",
    "    else:\n",
    "        y_oof_20_preds = []\n",
    "        fold_errors_20_preds =[]\n",
    "        avg_test_pred_20_preds = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        for ind, cv_folds in enumerate(big_cv_folds):\n",
    "            print('Fitting big fold', ind+1, 'out of', len(big_cv_folds))\n",
    "            y_oof = np.zeros((y.shape[0]))\n",
    "            fold_errors =[]\n",
    "            pred_test_list = []\n",
    "            \n",
    "            for i, (train_index, val_index) in enumerate(cv_folds):\n",
    "                print('Preprocessing fold ', i+1, 'out of ', len(cv_folds))\n",
    "                X_train, X_val  = X.iloc[train_index], X.iloc[val_index]\n",
    "                y_train, y_val = y[train_index], y[val_index]\n",
    "                \n",
    "                print('Fitting sub fold ', i+1, 'out of ', len(cv_folds))\n",
    "                # part to include additional functions\n",
    "                pred_test_y, pred_oof_log, clf = func_name(X_train, y_train, X_val, y_val, test, params)\n",
    "\n",
    "\n",
    "                y_oof[val_index] = pred_oof_log\n",
    "                curr_fe = np.sqrt(mean_squared_error(y_val, pred_oof_log))\n",
    "                print('Fold error ', curr_fe)\n",
    "                fold_errors.append(curr_fe)\n",
    "                pred_test_list.append(list(pred_test_y))\n",
    "\n",
    "            print('Total error', np.sqrt(mean_squared_error(y, y_oof)))\n",
    "            total_fe_std = round(np.std(fold_errors), 5)\n",
    "            print('Total std ',  total_fe_std)\n",
    "            avg_test_pred = np.mean(pred_test_list, axis=0)\n",
    "            \n",
    "            avg_test_pred_20_preds.append(avg_test_pred)\n",
    "            fold_errors_20_preds.append(fold_errors)\n",
    "            y_oof_20_preds.append(avg_test_pred)\n",
    "            \n",
    "        return y_oof_20_preds, avg_test_pred_20_preds, fold_errors_20_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "y = train_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(train_df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  preprocess(train_df.drop(columns=['ID', 'target']))\n",
    "test = preprocess(test_df.drop(columns=['ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_columns = list(pickle.load(open('good_columns.pkl', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 'all_good'\n",
    "cols = good_columns\n",
    "\n",
    "train['ts_last_mean_{}'.format(j)] = train_df[cols].apply(np.mean, axis=1)\n",
    "train['ts_last_median_{}'.format(j)] = train_df[cols].apply(np.median, axis=1)\n",
    "train['ts_last_sum_{}'.format(j)] = train_df[cols].apply(np.sum, axis=1)\n",
    "train['ts_last_nonzero_{}'.format(j)] = train_df[cols].apply(lambda x: np.mean([1 if v > 0 else 0 for v in x]), axis=1)\n",
    "train['ts_last_nonzero_mean_{}'.format(j)] = train_df[cols].apply(lambda x: np.mean([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "train['ts_last_nonzero_max_{}'.format(j)] = train_df[cols].apply(lambda x: np.max([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "train['ts_last_nonzero_min_{}'.format(j)] = train_df[cols].apply(lambda x: np.min([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "train['ts_last_nonzero_median_{}'.format(j)] = train_df[cols].apply(lambda x: np.median([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "\n",
    "\n",
    "test['ts_last_mean_{}'.format(j)] = test_df[cols].apply(np.mean, axis=1)\n",
    "test['ts_last_median_{}'.format(j)] = test_df[cols].apply(np.median, axis=1)\n",
    "test['ts_last_sum_{}'.format(j)] = test_df[cols].apply(np.sum, axis=1)\n",
    "test['ts_last_nonzero_{}'.format(j)] = test_df[cols].apply(lambda x: np.mean([1 if v > 0 else 0 for v in x]), axis=1)\n",
    "test['ts_last_nonzero_mean_{}'.format(j)] = test_df[cols].apply(lambda x: np.mean([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "test['ts_last_nonzero_max_{}'.format(j)] = test_df[cols].apply(lambda x: np.max([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "test['ts_last_nonzero_min_{}'.format(j)] = test_df[cols].apply(lambda x: np.min([v for v in x if v > 0]) if x.any() else 0, axis=1)\n",
    "test['ts_last_nonzero_median_{}'.format(j)] = test_df[cols].apply(lambda x: np.median([v for v in x if v > 0]) if x.any() else 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common(x, k):\n",
    "    c = Counter([j for j in x if j>0])\n",
    "    if c:\n",
    "        return c.most_common(1)[0][k]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['mos_common_v'.format(j)] = train_df.drop(columns=['ID', 'target']).apply(lambda x: get_most_common(x, 0), axis=1)\n",
    "train['mos_common_c'.format(j)] = train_df.drop(columns=['ID', 'target']).apply(lambda x: get_most_common(x, 1), axis=1)\n",
    "\n",
    "test['mos_common_v'.format(j)] = test_df.drop(columns=['ID']).apply(lambda x: get_most_common(x, 0), axis=1)\n",
    "test['mos_common_c'.format(j)] = test_df.drop(columns=['ID']).apply(lambda x: get_most_common(x, 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, cols in enumerate([cols0] + feature_groups[:50]):\n",
    "    train['unique_{}'.format(j)] = train_df[cols].apply(lambda x: len(set(x)), axis=1)\n",
    "    test['unique_{}'.format(j)] = test_df[cols].apply(lambda x: len(set(x)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[good_columns] =  train_df[good_columns]\n",
    "test[good_columns] =  test_df[good_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=good_columns, inplace=True)\n",
    "test.drop(columns=good_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_columns = list(pickle.load(open('good_columns.pkl', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting big fold 1 out of 20\n",
      "Preprocessing fold  1 out of  5\n",
      "Fitting sub fold  1 out of  5\n",
      "[0]\ttrain-rmse:13.2159\tval-rmse:13.2441\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 30 rounds.\n",
      "[30]\ttrain-rmse:2.55048\tval-rmse:2.56924\n",
      "[60]\ttrain-rmse:1.55223\tval-rmse:1.56974\n",
      "[90]\ttrain-rmse:1.42366\tval-rmse:1.45213\n",
      "[120]\ttrain-rmse:1.37386\tval-rmse:1.41259\n",
      "[150]\ttrain-rmse:1.34426\tval-rmse:1.39273\n",
      "[180]\ttrain-rmse:1.32356\tval-rmse:1.37951\n",
      "[210]\ttrain-rmse:1.30763\tval-rmse:1.37161\n",
      "[240]\ttrain-rmse:1.29462\tval-rmse:1.36569\n",
      "[270]\ttrain-rmse:1.28331\tval-rmse:1.36035\n",
      "[300]\ttrain-rmse:1.2733\tval-rmse:1.35673\n",
      "[330]\ttrain-rmse:1.26445\tval-rmse:1.35433\n",
      "[360]\ttrain-rmse:1.25626\tval-rmse:1.35142\n",
      "[390]\ttrain-rmse:1.24901\tval-rmse:1.34934\n",
      "[420]\ttrain-rmse:1.24223\tval-rmse:1.34806\n",
      "[450]\ttrain-rmse:1.23651\tval-rmse:1.34662\n",
      "[480]\ttrain-rmse:1.23051\tval-rmse:1.34526\n",
      "[510]\ttrain-rmse:1.22461\tval-rmse:1.34425\n",
      "[540]\ttrain-rmse:1.21887\tval-rmse:1.34319\n",
      "[570]\ttrain-rmse:1.21345\tval-rmse:1.34263\n",
      "[600]\ttrain-rmse:1.20829\tval-rmse:1.34237\n",
      "[630]\ttrain-rmse:1.20374\tval-rmse:1.34209\n",
      "[660]\ttrain-rmse:1.1993\tval-rmse:1.34146\n",
      "[690]\ttrain-rmse:1.19504\tval-rmse:1.3416\n",
      "Stopping. Best iteration:\n",
      "[664]\ttrain-rmse:1.19888\tval-rmse:1.34141\n",
      "\n",
      "Model training done in 4.674835205078125 seconds.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in fields ID",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ad6d2c79a572>\u001b[0m in \u001b[0;36mrun_calculations\u001b[0;34m(X, test, big_cv_folds, func_name, params)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting sub fold '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'out of '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;31m# part to include additional functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mpred_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_oof_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-82435c9e8de8>\u001b[0m in \u001b[0;36mrun_xgb\u001b[0;34m(train_X, train_y, val_X, val_y, test_X, params)\u001b[0m\n\u001b[1;32m     15\u001b[0m                       verbose_eval=30)\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model training done in {} seconds.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mxgb_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mpred_test_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpred_oof_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.72-py3.5.egg/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    263\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    264\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_pandas_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.72-py3.5.egg/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    184\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    185\u001b[0m Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\nDid not expect the data types in fields ID"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {'n_estimators':2000,\n",
    "             'max_depth':4, \n",
    "             'lambda':10000,\n",
    "             'eta': 0.3,\n",
    "            'min_child_weight':12,\n",
    "            \"colsample_bytree\":0.9,\n",
    "    #          \"tweedie_variance_power\":1.5,\n",
    "             'objective': 'reg:linear', \n",
    "             'eval_metric':'rmse',\n",
    "             'gamma':0.2}\n",
    "y_oof_lgb, pred_test_list_lgb, fold_errors = run_calculations(train, test, cv_folds, run_xgb, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test predictions: 20\n",
      "Length of avg test predictions: 49342\n"
     ]
    }
   ],
   "source": [
    "print('Length of test predictions:', len(pred_test_list_lgb))\n",
    "avg_pred_test_list_lgb = np.mean(pred_test_list_lgb, axis=0)\n",
    "print('Length of avg test predictions:', len(avg_pred_test_list_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERRORS\n",
    "errors = pd.DataFrame(fold_errors)\n",
    "errors.to_csv(os.path.join(PATH_TO_DATA, 'output/20_fold_errors_xgb_cv1348_std0025.csv'), index=False, header=False)\n",
    "\n",
    "# TRAIN TARGET OOF\n",
    "with open(os.path.join(PATH_TO_DATA, 'output/out_of_20_folds_xgb_cv1348_std0025.csv'), 'wb') as f:\n",
    "    pickle.dump(y_oof_lgb, f)\n",
    "\n",
    "# SUBMIT\n",
    "test = pd.read_csv(os.path.join(PATH_TO_DATA, 'test.csv'), usecols=['ID'])\n",
    "lgb = pd.DataFrame({'ID': test['ID'].values,\n",
    "                    'target': avg_pred_test_list_lgb})\n",
    "lgb.to_csv(os.path.join(PATH_TO_DATA, 'output/nefedov_xgb_cv1348_std0025.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
