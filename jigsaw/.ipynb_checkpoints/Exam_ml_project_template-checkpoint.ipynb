{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exam project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the template for the examination project for the machine learning course. You need to go through the template and fill in the code where necessary. Every step in the project explicitly specify the points you can get for solving it. If you complete all the steps, you will get more than 10 points (10 is the maximum grade for the exam), so you can choose which steps to solve or you can solve everything to insure that even if you make a mistake you will still get the maximum grade. \n",
    "\n",
    "The project in based on this competition - https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/ (you can get the data here - https://drive.google.com/drive/folders/1tne_sy_FhFuMBP_OY10sTuu5WQtlHV2b?usp=sharing )\n",
    "In this competetion you need to detect toxic comment (binary classification). You are given training data in english and need to detect toxicity in other languages.\n",
    "\n",
    "However, you are not required to follow the rules of the competetion. You can use only the English data from the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data, load it into jupyter notebook and descriptevly analyze the data.\n",
    "\n",
    "Use jigsaw-unintended-bias-train.csv or jigsaw-toxic-comment-train.csv (they are a bit different, but it doesn't matter here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('jigsaw-unintended-bias-train.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                       comment_text     toxic  \\\n",
       "0  59848  This is so cool. It's like, 'would you want yo...  0.000000   \n",
       "1  59849  Thank you!! This would make my life a lot less...  0.000000   \n",
       "2  59852  This is such an urgent design problem; kudos t...  0.000000   \n",
       "3  59855  Is this something I'll be able to install on m...  0.000000   \n",
       "4  59856               haha you guys are a bunch of losers.  0.893617   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "             ...             article_id    rating  funny  wow  sad  likes  \\\n",
       "0            ...                   2006  rejected      0    0    0      0   \n",
       "1            ...                   2006  rejected      0    0    0      0   \n",
       "2            ...                   2006  rejected      0    0    0      0   \n",
       "3            ...                   2006  rejected      0    0    0      0   \n",
       "4            ...                   2006  rejected      0    0    0      1   \n",
       "\n",
       "   disagree  sexual_explicit  identity_annotator_count  \\\n",
       "0         0              0.0                         0   \n",
       "1         0              0.0                         0   \n",
       "2         0              0.0                         0   \n",
       "3         0              0.0                         0   \n",
       "4         0              0.0                         4   \n",
       "\n",
       "   toxicity_annotator_count  \n",
       "0                         4  \n",
       "1                         4  \n",
       "2                         4  \n",
       "3                         4  \n",
       "4                        47  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task  - 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the distribution of the target variable (toxic). Is the dataset imbalanced? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code (make a histogram) and comment here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the longest and the shortest toxic comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (select the toxic comments, find the longest and select the longest)\n",
    "# your code here (select the toxic comments, find the shortest and select the shortest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a couple of really weird comments (e.g. only upper-cased letters, one word repeated many times etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra task - 0.5 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the distribution of two non-target sentiment variables (e.g. insult, identity_attack). Find a couple of examples with a high positive label in these columns (select the worst insults for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze how different are the comments that are labeled positive in one column and in the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "# add descriptive statistics to the dataset\n",
    "data['comment_length'] = data['comment_text'].apply(len) # for example\n",
    "\n",
    "# round the sentiment columns, groupby() by these two columns, use .agg() to compute descriptive statistics \n",
    "# you can use length of the comment, number of ! marks in the comment, number of upper-cased letter etc\n",
    "\n",
    "# Use at least 5 statistics for comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - 0.5 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a couple of comments with the maximum number of positive sentiment labels (e.g. insulting, identity atticking, asian-targeted ... at the same time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code and examples here (create a function to combine all the sentiment scores into one score and sort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find three words that are often used in the toxic comments and rarely used in the non-toxic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here (collect word statistics, create a scoring function that \n",
    "# measures how specific a word is to a certain class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the data to train a baseline model. You can downsample the data if you can't handle that much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(frac=0.1) # e.g. take only a 10 % of original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task  - 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the comments using CountVectorizer of TfidfVectorizer from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data (vectorized texts and target variable) into a train and a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a classifier on the train matrix and evaluate on the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use any of these classifiers\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# use SGDClassifier with different parameters if you want to keep the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ...\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "preds = clf.predict(X_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_valid, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to impove the score by tuning the parameters in the vectorizer and in the classifier (or try other classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exta task - 0.5 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of train_test_split use cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here (check if your best model with tuned hyperparameters performs well using proper validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra task - 0.5 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try grid search instead of manually tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to optimize at least 5 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra task - 0.5 point for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a more sophisticated model with stratified cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these models\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune at least 5 parameters in each of the chosen models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Competion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your model on the validation.csv (multilingual). How bad does it perform compared to the intrinsic evaluation (using test split or cross-validation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main task - 1.5 point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to improve your model so it performs reasonably well on the multilingual validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (try at least 5 hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial on eli5 - https://eli5.readthedocs.io/en/latest/tutorials/sklearn-text.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main task - 1 point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most important features in one of your models. Use model specific methods (model.feature_importances, model.coef_), l1-regularization or eli5 library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "# list top most important 25 features (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra task - 0.5 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find uninformative features and remove them (you can add these words as stopswords to vectorizer, for example). Retrain your model without these uninformative features. Did the quality of your model improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Gradient boosting and ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefull links:\n",
    "\n",
    "LightGBM for text classification example - https://www.kaggle.com/sreeram004/simple-lightgbm-classifier  \n",
    "Another lightgbm example (see the replies) - https://stackoverflow.com/questions/50250432/python-lightgbm-text-classicication-with-tfidf  \n",
    "\n",
    "Xgboost -  https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0   \n",
    "Catboost - https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main task - 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a gradient boosting model using Xgboost, CatBoost or LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra task - 1 point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune hyperparameters of your gradient boosting model (at least 5 parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra task - 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an ensemble model (using voting classifier for example) using simpler models from sklearn (at least 5 models).\n",
    "\n",
    "Tutorial on ensemble methods  - https://colab.research.google.com/drive/1XHAB6SjzW2WIoQuaDsm0IwsK963GLkhr#scrollTo=gw8H_0cx8OK0  (in Russian but code is self-sufficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefull links:  \n",
    "Introduction - https://github.com/mannefedov/hse_ml_m1/blob/master/6_nn_intro/nn_intro.ipynb (Russian)  \n",
    "CNN -  https://github.com/mannefedov/hse_ml_m1/tree/master/7_cnn (Russian)  \n",
    "RNN - https://github.com/mannefedov/hse_ml_m1/blob/master/8_rnn/RNN.ipynb (Russian)  \n",
    "\n",
    "Keras tutorials:  \n",
    "https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py   \n",
    "https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py  \n",
    "https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main task - 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an LSTM model on the data and evaluate it using train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = text.lower().split()\n",
    "    tokens = [token.strip(punctuation) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the dictionary\n",
    "vocab = Counter()\n",
    "\n",
    "for text in data.comment_text:\n",
    "    vocab.update(preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter rare words\n",
    "filtered_vocab = set()\n",
    "\n",
    "for word in vocab:\n",
    "    if vocab[word] > 30:\n",
    "        filtered_vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add special tags for padding and unk words\n",
    "# create word to index mapping\n",
    "word2id = {'UNK':1, 'PAD':0}\n",
    "\n",
    "for word in filtered_vocab:\n",
    "    word2id[word] = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the texts to sequences of indicies\n",
    "X = []\n",
    "\n",
    "for text in data.comment_text:\n",
    "    tokens = preprocess(text)\n",
    "    ids = [word2id.get(token, 1) for token in tokens]\n",
    "    X.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max(len(x) for x in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your model here\n",
    "model = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_valid, y_valid),\n",
    "          batch_size=128,\n",
    "         epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra task - 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Convolutional model on character instead of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra task - 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pretrained embedding model in your neural network (word2vec or fastext, which you can train on the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word to embedding mapping\n",
    "weights = np.zeros((len(word2id), 100))\n",
    "\n",
    "for word, i in word2id.items():\n",
    "    if word == 'PAD':\n",
    "        continue\n",
    "    try:\n",
    "        weights[i] = ft[word]\n",
    "    \n",
    "    except KeyError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your model here\n",
    "inputs = tf.keras.layers.Input()\n",
    "# your can pass pretrained embedding like that \n",
    "embeddings = tf.keras.layers.Embedding(input_dim=len(word2id), output_dim=, trainable=False,\n",
    "                                      weights=[weights])(inputs, )\n",
    "# continue your modek here\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usefull links:\n",
    "\n",
    "https://github.com/mannefedov/hse_ml_m1/blob/master/4_clustering/4_clustering.ipynb (Russian)\n",
    "\n",
    "https://sanjayasubedi.com.np/nlp/nlp-with-python-document-clustering/ (English)\n",
    "\n",
    "https://nlpforhackers.io/recipe-text-clustering/ (English)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main task - 2 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the comments, analyze the cluster (are the comments with the same toxicity in the same clusters, what are the largest clusters, are there any interisting outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
