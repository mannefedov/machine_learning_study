{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: This kernel provides some experiments/updates starting from a seminal public kernel by [Chris Deotte](https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a TensorFlow starter notebook for Kaggle's \"Feedback Prize - Evaluating Student Writing\" Competition. Currently this notebook uses\n",
    "* backbone LongFormer\n",
    "* Named Entity Recognition (NER) formulation\n",
    "* one fold\n",
    "\n",
    "With simple changes, we can convert this notebook into Question Answer formulation and we can try different backbones. Furthermore this notebook is one fold. It trains with 90% data and validates on 10% data. We can convert this notebook to K-fold or train with 100% data for boost in LB.\n",
    "\n",
    "The transformer model LongFormer is explained [here][1]. It is similar to Roberta but can accept inputs as wide as 4096 tokens! In this notebook we feed the transformer with 1024 wide tokens. HuggingFace user AllenAI uploaded pretrained weights for us [here][2]\n",
    "\n",
    "[1]: https://huggingface.co/docs/transformers/model_doc/longformer\n",
    "[2]: https://huggingface.co/allenai/longformer-base-4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "This notebook can either train a new model or load a previously trained model (made from previous notebook version). Furthermore, this notebook can either create new NER tokens or load existing tokens (made from previous notebook version). In this notebook version, we will load model and load NER tokens. \n",
    "\n",
    "Also this notebook can load huggingface stuff (like tokenizers) from a Kaggle dataset, or download it from internet. (If it downloads from internet, you can then put it in a Kaggle dataset, so next time you can turn internet off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip feedback-prize-2021.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:12.849936Z",
     "iopub.status.busy": "2022-02-17T14:58:12.849415Z",
     "iopub.status.idle": "2022-02-17T14:58:12.857323Z",
     "shell.execute_reply": "2022-02-17T14:58:12.856592Z",
     "shell.execute_reply.started": "2022-02-17T14:58:12.849898Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# DECLARE HOW MANY GPUS YOU WISH TO USE. \n",
    "# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n",
    "\n",
    "# VERSION FOR SAVING MODEL WEIGHTS\n",
    "VER=12\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n",
    "# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\n",
    "LOAD_TOKENS_FROM = None\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n",
    "# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\n",
    "LOAD_MODEL_FROM = None\n",
    "\n",
    "# IF FOLLOWING IS NONE, THEN NOTEBOOK \n",
    "# USES INTERNET AND DOWNLOADS HUGGINGFACE \n",
    "# CONFIG, TOKENIZER, AND MODEL\n",
    "DOWNLOADED_MODEL_PATH = None\n",
    "if DOWNLOADED_MODEL_PATH is None:\n",
    "    DOWNLOADED_MODEL_PATH = 'model'    \n",
    "# MODEL_NAME = 'distilroberta-base'\n",
    "MODEL_NAME = 'allenai/longformer-base-4096'\n",
    "# MODEL_NAME = 'allenai/led-large-16384-arxiv'\n",
    "# MODEL_NAME = 'jplu/tf-xlm-roberta-base'\n",
    "# MODEL_NAME = 'allenai/led-base-16384'\n",
    "# MODEL_NAME = 'xlnet-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Submit TensorFlow Without Internet\n",
    "Many people ask me, how do I submit TensorFlow models without internet? With HuggingFace Transformer, it's easy. Just download the following 3 things (1) model weights, (2) tokenizer files, (3) config file, and upload them to a Kaggle dataset. Below shows code how to get the files from HuggingFace for AllenAI's model `longformer-base`. But this same code can download any transformer, like for example `roberta-base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:13.500244Z",
     "iopub.status.busy": "2022-02-17T14:58:13.499755Z",
     "iopub.status.idle": "2022-02-17T14:58:13.505196Z",
     "shell.execute_reply": "2022-02-17T14:58:13.50451Z",
     "shell.execute_reply.started": "2022-02-17T14:58:13.500205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "2022-02-19 08:04:54.254756: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-19 08:04:54.811085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22311 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:82:00.0, compute capability: 8.6\n",
      "2022-02-19 08:04:55.760738: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at allenai/longformer-base-4096.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if DOWNLOADED_MODEL_PATH == 'model':\n",
    "    from transformers import AutoTokenizer, AutoConfig, TFAutoModel\n",
    "\n",
    "#     os.mkdir('model')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.save_pretrained('model')\n",
    "\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME) \n",
    "    config.save_pretrained('model')\n",
    "\n",
    "    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n",
    "    backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above saves the files\n",
    "* TOKENIZER FILES - merges.txt, tokenizer_config.json, special_tokens_map.json, tokenizer.json, vocab.json\n",
    "* CONFIG FILE - config.json\n",
    "* MODEL WEIGHT FILE - tf_model.h5\n",
    "\n",
    "Then just upload all these files to a Kaggle dataset, like what I did [here][1]. Then you load them into your notebook like the notebook you are reading. And we can turn internet off!\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/tf-longformer-v12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas matplotlib sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:14.142138Z",
     "iopub.status.busy": "2022-02-17T14:58:14.141445Z",
     "iopub.status.idle": "2022-02-17T14:58:24.695762Z",
     "shell.execute_reply": "2022-02-17T14:58:24.694269Z",
     "shell.execute_reply.started": "2022-02-17T14:58:14.142102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #disable all tensorflow logging output\n",
    "\n",
    "from transformers import *\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:24.697979Z",
     "iopub.status.busy": "2022-02-17T14:58:24.697731Z",
     "iopub.status.idle": "2022-02-17T14:58:24.711746Z",
     "shell.execute_reply": "2022-02-17T14:58:24.710975Z",
     "shell.execute_reply.started": "2022-02-17T14:58:24.697941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single strategy\n"
     ]
    }
   ],
   "source": [
    "# USE MULTIPLE GPUS\n",
    "if os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('single strategy')\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('multiple strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:24.714268Z",
     "iopub.status.busy": "2022-02-17T14:58:24.713693Z",
     "iopub.status.idle": "2022-02-17T14:58:24.723603Z",
     "shell.execute_reply": "2022-02-17T14:58:24.72279Z",
     "shell.execute_reply.started": "2022-02-17T14:58:24.714231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision enabled\n"
     ]
    }
   ],
   "source": [
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "print('Mixed precision enabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:24.72642Z",
     "iopub.status.busy": "2022-02-17T14:58:24.726209Z",
     "iopub.status.idle": "2022-02-17T14:58:26.139158Z",
     "shell.execute_reply": "2022-02-17T14:58:26.137693Z",
     "shell.execute_reply.started": "2022-02-17T14:58:24.726394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144293, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "print( train.shape )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:26.140978Z",
     "iopub.status.busy": "2022-02-17T14:58:26.140714Z",
     "iopub.status.idle": "2022-02-17T14:58:28.505315Z",
     "shell.execute_reply": "2022-02-17T14:58:28.504551Z",
     "shell.execute_reply.started": "2022-02-17T14:58:26.140938Z"
    }
   },
   "outputs": [],
   "source": [
    "# WE ASSUME DATAFRAME IS ASCENDING WHICH IT IS\n",
    "assert( np.sum(train.groupby('id')['discourse_start'].diff()<=0)==0 ) # assert showes no error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.506774Z",
     "iopub.status.busy": "2022-02-17T14:58:28.506526Z",
     "iopub.status.idle": "2022-02-17T14:58:28.526768Z",
     "shell.execute_reply": "2022-02-17T14:58:28.526049Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.50674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train labels are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n",
       "       'Counterclaim', 'Rebuttal'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The train labels are:')\n",
    "train.discourse_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.528257Z",
     "iopub.status.busy": "2022-02-17T14:58:28.528012Z",
     "iopub.status.idle": "2022-02-17T14:58:28.546506Z",
     "shell.execute_reply": "2022-02-17T14:58:28.54586Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.528225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15594 train texts.\n"
     ]
    }
   ],
   "source": [
    "IDS = train.id.unique()\n",
    "print('There are',len(IDS),'train texts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Train\n",
    "The following code converts Kaggle's train dataset into a NER token array that we can use to train a NER transformer. I have made it very clear which targets belong to which class. This allows us to very easily convert this code to `Question Answer formulation` if we want. Just change the 14 NER arrays to be 14 arrays of `start position` and `end position` for each of the 7 classes. (You will need to think creatively what to do if a single text has multiple of one class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.547902Z",
     "iopub.status.busy": "2022-02-17T14:58:28.547603Z",
     "iopub.status.idle": "2022-02-17T14:58:28.696916Z",
     "shell.execute_reply": "2022-02-17T14:58:28.696133Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.547865Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "\n",
    "# THE TOKENS AND ATTENTION ARRAYS\n",
    "tokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH)\n",
    "train_lens = []\n",
    "train_tokens = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
    "train_attention = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
    "\n",
    "# THE 14 CLASSES FOR NER\n",
    "lead_b = np.zeros((len(IDS),MAX_LEN))\n",
    "lead_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "position_b = np.zeros((len(IDS),MAX_LEN))\n",
    "position_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "evidence_b = np.zeros((len(IDS),MAX_LEN))\n",
    "evidence_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "claim_b = np.zeros((len(IDS),MAX_LEN))\n",
    "claim_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "conclusion_b = np.zeros((len(IDS),MAX_LEN))\n",
    "conclusion_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "counterclaim_b = np.zeros((len(IDS),MAX_LEN))\n",
    "counterclaim_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "rebuttal_b = np.zeros((len(IDS),MAX_LEN))\n",
    "rebuttal_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "# HELPER VARIABLES\n",
    "targets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\n",
    "targets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]\n",
    "target_map = {'Lead':0, 'Position':1, 'Evidence':2, 'Claim':3, 'Concluding Statement':4,\n",
    "             'Counterclaim':5, 'Rebuttal':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.698437Z",
     "iopub.status.busy": "2022-02-17T14:58:28.698151Z",
     "iopub.status.idle": "2022-02-17T14:58:28.710313Z",
     "shell.execute_reply": "2022-02-17T14:58:28.70951Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.698395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , 7800 , 7900 , 8000 , 8100 , 8200 , 8300 , 8400 , 8500 , 8600 , 8700 , 8800 , 8900 , 9000 , 9100 , 9200 , 9300 , 9400 , 9500 , 9600 , 9700 , 9800 , 9900 , 10000 , 10100 , 10200 , 10300 , 10400 , 10500 , 10600 , 10700 , 10800 , 10900 , 11000 , 11100 , 11200 , 11300 , 11400 , 11500 , 11600 , 11700 , 11800 , 11900 , 12000 , 12100 , 12200 , 12300 , 12400 , 12500 , 12600 , 12700 , 12800 , 12900 , 13000 , 13100 , 13200 , 13300 , 13400 , 13500 , 13600 , 13700 , 13800 , 13900 , 14000 , 14100 , 14200 , 14300 , 14400 , 14500 , 14600 , 14700 , 14800 , 14900 , 15000 , 15100 , 15200 , 15300 , 15400 , 15500 , "
     ]
    }
   ],
   "source": [
    "# FOR LOOP THROUGH EACH TRAIN TEXT\n",
    "for id_num in range(len(IDS)):\n",
    "    if LOAD_TOKENS_FROM: break\n",
    "    if id_num%100==0: print(id_num,', ',end='')\n",
    "        \n",
    "    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n",
    "    n = IDS[id_num]\n",
    "    name = f'train/{n}.txt'\n",
    "    txt = open(name, 'r').read()\n",
    "    train_lens.append( len(txt.split()))\n",
    "    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "    train_tokens[id_num,] = tokens['input_ids'] # keys from words in text\n",
    "    train_attention[id_num,] = tokens['attention_mask'] # 1 for each word in text, 0 for padding\n",
    "    \n",
    "    # FIND TARGETS IN TEXT AND SAVE IN TARGET ARRAYS\n",
    "    offsets = tokens['offset_mapping'] # position (start, end) of each word, including special characters such as \\n\n",
    "    offset_index = 0\n",
    "    df = train.loc[train.id==n] # extract train data related to actual text\n",
    "    for index,row in df.iterrows():\n",
    "        a = row.discourse_start\n",
    "        b = row.discourse_end\n",
    "        \n",
    "        if offset_index>len(offsets)-1:\n",
    "            break\n",
    "            \n",
    "        c = offsets[offset_index][0]\n",
    "        d = offsets[offset_index][1]\n",
    "        \n",
    "        beginning = True\n",
    "        \n",
    "        while b>c:\n",
    "            if (c>=a)&(b>=d): # word is in the target\n",
    "                k = target_map[row.discourse_type]\n",
    "                if beginning:\n",
    "                    targets_b[k][id_num][offset_index] = 1 \n",
    "                    beginning = False\n",
    "                else:\n",
    "                    targets_i[k][id_num][offset_index] = 1 \n",
    "            offset_index += 1\n",
    "            if offset_index>len(offsets)-1:\n",
    "                break\n",
    "            c = offsets[offset_index][0]\n",
    "            d = offsets[offset_index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.713498Z",
     "iopub.status.busy": "2022-02-17T14:58:28.71316Z",
     "iopub.status.idle": "2022-02-17T14:58:28.721296Z",
     "shell.execute_reply": "2022-02-17T14:58:28.720612Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.71347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if LOAD_TOKENS_FROM is None:\n",
    "    plt.hist(train_lens,bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "From the histogram of train token counts above, we see that using a transformer width of 1024 is a good comprise of capturing most of the data's signal but not having too large a model. We could probably explore other widths between 512 and 1024 also. Or we could use widths of size 512 or smaller and use a stride which breaks a single text into multiple chunks (with possible overlap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.722952Z",
     "iopub.status.busy": "2022-02-17T14:58:28.722707Z",
     "iopub.status.idle": "2022-02-17T14:58:28.730592Z",
     "shell.execute_reply": "2022-02-17T14:58:28.729882Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.722918Z"
    }
   },
   "outputs": [],
   "source": [
    "if LOAD_TOKENS_FROM is None:\n",
    "    targets = np.zeros((len(IDS),MAX_LEN,15), dtype='int32')\n",
    "    for k in range(7):\n",
    "        targets[:,:,2*k] = targets_b[k]\n",
    "        targets[:,:,2*k+1] = targets_i[k]\n",
    "    targets[:,:,14] = 1-np.max(targets,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.733221Z",
     "iopub.status.busy": "2022-02-17T14:58:28.73303Z",
     "iopub.status.idle": "2022-02-17T14:58:36.824137Z",
     "shell.execute_reply": "2022-02-17T14:58:36.8234Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.733196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved NER tokens\n"
     ]
    }
   ],
   "source": [
    "if LOAD_TOKENS_FROM is None:\n",
    "    np.save(f'targets_{MAX_LEN}', targets)\n",
    "    np.save(f'tokens_{MAX_LEN}', train_tokens)\n",
    "    np.save(f'attention_{MAX_LEN}', train_attention)\n",
    "    print('Saved NER tokens')\n",
    "else:\n",
    "    targets = np.load(f'{LOAD_TOKENS_FROM}/targets_{MAX_LEN}.npy')\n",
    "    train_tokens = np.load(f'{LOAD_TOKENS_FROM}/tokens_{MAX_LEN}.npy')\n",
    "    train_attention = np.load(f'{LOAD_TOKENS_FROM}/attention_{MAX_LEN}.npy')\n",
    "    print('Loaded NER tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "We will use LongFormer backbone and add our own NER head using one hidden layer of size 256 and one final layer with softmax. We use 15 classes because we have a `B` class and `I` class for each of 7 labels. And we have an additional class (called `O` class) for tokens that do not belong to one of the 14 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "#     def __init__(self, d_model, warmup_steps=500):\n",
    "#         super(CustomSchedule, self).__init__()\n",
    "\n",
    "#         self.d_model = d_model\n",
    "#         self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "#         self.warmup_steps = warmup_steps\n",
    "\n",
    "#     def __call__(self, step):\n",
    "#         arg1 = tf.math.rsqrt(step)\n",
    "#         arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "# learning_rate = CustomSchedule(20000)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(\n",
    "#     learning_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-8, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundaries = [500, 3000]\n",
    "# values = [1e-4, 1e-5, 1e-6]\n",
    "# learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "#     boundaries, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:36.825689Z",
     "iopub.status.busy": "2022-02-17T14:58:36.825443Z",
     "iopub.status.idle": "2022-02-17T14:58:36.834316Z",
     "shell.execute_reply": "2022-02-17T14:58:36.833138Z",
     "shell.execute_reply.started": "2022-02-17T14:58:36.825654Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n",
    "    attention = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'attention', dtype=tf.int32)\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \n",
    "    backbone = TFAutoModel.from_pretrained(DOWNLOADED_MODEL_PATH+'/tf_model.h5', config=config, trainable=True)\n",
    "    \n",
    "    x = backbone(tokens, attention_mask=attention)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x[0])\n",
    "    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[tokens,attention], outputs=x)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.12e-4),\n",
    "                  loss =tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:36.83675Z",
     "iopub.status.busy": "2022-02-17T14:58:36.835892Z",
     "iopub.status.idle": "2022-02-17T14:59:07.544181Z",
     "shell.execute_reply": "2022-02-17T14:59:07.542427Z",
     "shell.execute_reply.started": "2022-02-17T14:58:36.836712Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFLongformerModel.\n",
      "\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at model/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train or Load Model\n",
    "If you provide a path in variable `LOAD_MODEL_FROM` above, then it will load your previously trained model. Otherwise it will train now. \n",
    "\n",
    "We train 5 epochs of batch size 32 using learning rate `1e-4` for the first four and `1e-5` for the last epoch. I trained my model offline. If you wish to train on Kaggle's GPU, we may need to reduce the batch size. If we reduce the batch size to 8. That is 1/4 original. So we should also reduce the learning rates to `0.25e-4` and `0.25e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:59:07.546602Z",
     "iopub.status.busy": "2022-02-17T14:59:07.546263Z",
     "iopub.status.idle": "2022-02-17T14:59:07.558327Z",
     "shell.execute_reply": "2022-02-17T14:59:07.557543Z",
     "shell.execute_reply.started": "2022-02-17T14:59:07.546564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 14034 , Valid size 1560\n"
     ]
    }
   ],
   "source": [
    "# TRAIN VALID SPLIT 90% 10%\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "np.random.seed(None)\n",
    "print('Train size',len(train_idx),', Valid size',len(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:59:07.560171Z",
     "iopub.status.busy": "2022-02-17T14:59:07.559917Z",
     "iopub.status.idle": "2022-02-17T14:59:07.565073Z",
     "shell.execute_reply": "2022-02-17T14:59:07.564214Z",
     "shell.execute_reply.started": "2022-02-17T14:59:07.560139Z"
    }
   },
   "outputs": [],
   "source": [
    "# # LEARNING RATE SCHEDULE AND MODEL CHECKPOINT\n",
    "EPOCHS = 5\n",
    "LRS = [0.12e-4, 0.12e-4, 0.12e-4, 0.12e-5, 0.12e-6]\n",
    "def lrfn(epoch):\n",
    "    return LRS[epoch]\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:59:07.567096Z",
     "iopub.status.busy": "2022-02-17T14:59:07.56675Z",
     "iopub.status.idle": "2022-02-17T15:17:20.069387Z",
     "shell.execute_reply": "2022-02-17T15:17:20.068226Z",
     "shell.execute_reply.started": "2022-02-17T14:59:07.567061Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # LOAD MODEL\n",
    "# if LOAD_MODEL_FROM:\n",
    "#     model.load_weights(f'{LOAD_MODEL_FROM}/long_v14.h5')\n",
    "    \n",
    "# # OR TRAIN MODEL\n",
    "# else:\n",
    "#     model.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n",
    "#           y = targets[train_idx,],\n",
    "#           validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n",
    "#                              targets[valid_idx,]),\n",
    "#           callbacks = [lr_callback],\n",
    "#           epochs = 5,\n",
    "#           batch_size = 4,\n",
    "#           verbose = 1)\n",
    "\n",
    "#     # SAVE MODEL WEIGHTS\n",
    "#     model.save_weights(f'long_v{VER}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(dataset='train', verbose=True, text_ids=IDS[valid_idx], preds=oof_preds):\n",
    "    all_predictions = []\n",
    "\n",
    "    for id_num in range(len(preds)):\n",
    "    \n",
    "        # GET ID\n",
    "        if (id_num%100==0)&(verbose): \n",
    "            print(id_num,', ',end='')\n",
    "        n = text_ids[id_num]\n",
    "    \n",
    "        # GET TOKEN POSITIONS IN CHARS\n",
    "        name = f'{dataset}/{n}.txt'\n",
    "        txt = open(name, 'r').read()\n",
    "        \n",
    "        '''txt = txt.replace(' .', '. ')\n",
    "        txt = txt.replace(' ,', ', ')'''\n",
    "        \n",
    "        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "        off = tokens['offset_mapping']\n",
    "    \n",
    "        # GET WORD POSITIONS IN CHARS\n",
    "        w = []\n",
    "        blank = True\n",
    "        for i in range(len(txt)):\n",
    "            if (txt[i]!=' ')&(txt[i]!='\\n')&(txt[i]!='\\xa0')&(txt[i]!='\\x85')&(blank==True):\n",
    "                w.append(i)\n",
    "                blank=False\n",
    "            elif (txt[i]==' ')|(txt[i]=='\\n')|(txt[i]=='\\xa0')|(txt[i]=='\\x85'):\n",
    "                blank=True\n",
    "        w.append(1e6)\n",
    "            \n",
    "        # MAPPING FROM TOKENS TO WORDS\n",
    "        word_map = -1 * np.ones(MAX_LEN,dtype='int32')\n",
    "        w_i = 0\n",
    "        for i in range(len(off)):\n",
    "            if off[i][1]==0: continue #ignore first blank\n",
    "            while off[i][0]>=w[w_i+1]: w_i += 1\n",
    "            word_map[i] = int(w_i)\n",
    "        \n",
    "        # CONVERT TOKEN PREDICTIONS INTO WORD LABELS\n",
    "        # KEY:\n",
    "        # 0: LEAD_B, 1: LEAD_I\n",
    "        # 2: POSITION_B, 3: POSITION_I\n",
    "        # 4: EVIDENCE_B, 5: EVIDENCE_I\n",
    "        # 6: CLAIM_B, 7: CLAIM_I\n",
    "        # 8: CONCLUSION_B, 9: CONCLUSION_I\n",
    "        # 10: COUNTERCLAIM_B, 11: COUNTERCLAIM_I\n",
    "        # 12: REBUTTAL_B, 13: REBUTTAL_I\n",
    "        # 14: NOTHING i.e. O\n",
    "        pred = preds[id_num,]/2.0\n",
    "    \n",
    "        i = 0\n",
    "        while i<MAX_LEN:\n",
    "            prediction = []\n",
    "            start = pred[i]\n",
    "            if start in [0,1,2,3,4,5,6,7]:\n",
    "                prediction.append(word_map[i])\n",
    "                i += 1\n",
    "                if i>=MAX_LEN: break\n",
    "                while pred[i]==start+0.5:\n",
    "                    if not word_map[i] in prediction:\n",
    "                        prediction.append(word_map[i])\n",
    "                    i += 1\n",
    "                    if i>=MAX_LEN: break\n",
    "            else:\n",
    "                i += 1\n",
    "            prediction = [x for x in prediction if x!=-1]\n",
    "            if len(prediction)>=4:\n",
    "                all_predictions.append( (n, target_map_rev[int(start)], \n",
    "                                ' '.join([str(x) for x in prediction]) ) )\n",
    "                \n",
    "    # MAKE DATAFRAME\n",
    "    df = pd.DataFrame(all_predictions)\n",
    "    df.columns = ['id','class','predictionstring']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FROM : Rob Mulla @robikscube\n",
    "# https://www.kaggle.com/robikscube/student-writing-competition-twitch\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(' '))\n",
    "    set_gt = set(row.predictionstring_gt.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter/ len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id','class'],\n",
    "                           right_on=['id','discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
    "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
    "\n",
    "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1.2e-05.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_7/longformer/pooler/dense/kernel:0', 'tf_longformer_model_7/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_longformer_model_7/longformer/pooler/dense/kernel:0', 'tf_longformer_model_7/longformer/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 15:10:16.241172: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2172] Converted 4944/23242 nodes to float16 precision using 593 cast(s) to float16 (excluding Const and Variable casts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3509/3509 [==============================] - ETA: 0s - loss: 0.3732 - categorical_accuracy: 0.8831"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 15:37:06.766251: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2172] Converted 2095/4335 nodes to float16 precision using 111 cast(s) to float16 (excluding Const and Variable casts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "3509/3509 [==============================] - 1707s 469ms/step - loss: 0.3732 - categorical_accuracy: 0.8831 - val_loss: 0.3083 - val_categorical_accuracy: 0.8996 - lr: 1.2000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 15:38:06.242561: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2172] Converted 2403/6645 nodes to float16 precision using 124 cast(s) to float16 (excluding Const and Variable casts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 - 78s - 78s/epoch - 793ms/step\n",
      "OOF predictions shape: (1560, 1024, 15)\n",
      "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , Lead 0.5660138976626659\n",
      "Evidence 0.2504996668887408\n",
      "Concluding Statement 0.39499455930359084\n",
      "Claim 0.033224990971469845\n",
      "Position 0.012690355329949238\n",
      "\n",
      "Overall 0.2514846940312834\n",
      "1\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1.2e-05.\n",
      "3509/3509 [==============================] - 1635s 466ms/step - loss: 0.2931 - categorical_accuracy: 0.9037 - val_loss: 0.2924 - val_categorical_accuracy: 0.9040 - lr: 1.2000e-05\n",
      "98/98 - 64s - 64s/epoch - 651ms/step\n",
      "OOF predictions shape: (1560, 1024, 15)\n",
      "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , Lead 0.5660138976626659\n",
      "Evidence 0.2504996668887408\n",
      "Concluding Statement 0.39499455930359084\n",
      "Claim 0.033224990971469845\n",
      "Position 0.012690355329949238\n",
      "\n",
      "Overall 0.2514846940312834\n",
      "2\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1.2e-05.\n",
      "3509/3509 [==============================] - 1636s 466ms/step - loss: 0.2657 - categorical_accuracy: 0.9110 - val_loss: 0.2871 - val_categorical_accuracy: 0.9052 - lr: 1.2000e-05\n",
      "98/98 - 64s - 64s/epoch - 651ms/step\n",
      "OOF predictions shape: (1560, 1024, 15)\n",
      "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , Lead 0.5660138976626659\n",
      "Evidence 0.2504996668887408\n",
      "Concluding Statement 0.39499455930359084\n",
      "Claim 0.033224990971469845\n",
      "Position 0.012690355329949238\n",
      "\n",
      "Overall 0.2514846940312834\n",
      "3\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1.2e-05.\n",
      " 146/3509 [>.............................] - ETA: 25:23 - loss: 0.2342 - categorical_accuracy: 0.9205"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_attention\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_attention\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlr_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# SAVE MODEL WEIGHTS\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlong_v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i)\n",
    "    model.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n",
    "          y = targets[train_idx,],\n",
    "          validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n",
    "                             targets[valid_idx,]),\n",
    "          callbacks = [lr_callback],\n",
    "          epochs = 1,\n",
    "          batch_size = 4,\n",
    "          verbose = 1)\n",
    "\n",
    "    # SAVE MODEL WEIGHTS\n",
    "    model.save_weights(f'long_v{i}.h5')\n",
    "    p = model.predict([train_tokens[valid_idx,], train_attention[valid_idx,]], \n",
    "                  batch_size=16, verbose=2)\n",
    "    print('OOF predictions shape:',p.shape)\n",
    "    oof_preds = np.argmax(p,axis=-1)\n",
    "    target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n",
    "             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}\n",
    "    \n",
    "    oof = get_preds( dataset='train', verbose=True, text_ids=IDS[valid_idx] )\n",
    "    oof.head()\n",
    "    \n",
    "    # VALID DATAFRAME\n",
    "    valid = train.loc[train['id'].isin(IDS[valid_idx])]\n",
    "    f1s = []\n",
    "    CLASSES = oof['class'].unique()\n",
    "    for c in CLASSES:\n",
    "        pred_df = oof.loc[oof['class']==c].copy()\n",
    "        gt_df = valid.loc[valid['discourse_type']==c].copy()\n",
    "        f1 = score_feedback_comp(pred_df, gt_df)\n",
    "        print(c,f1)\n",
    "        f1s.append(f1)\n",
    "    print()\n",
    "    print('Overall',np.mean(f1s))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:17:39.725762Z",
     "iopub.status.busy": "2022-02-17T15:17:39.725499Z",
     "iopub.status.idle": "2022-02-17T15:17:41.046651Z",
     "shell.execute_reply": "2022-02-17T15:17:41.045901Z",
     "shell.execute_reply.started": "2022-02-17T15:17:39.725733Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.save_weights(f'long_v{VER}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Model - Infer Out of fold (OOF)\n",
    "We will now make predictions on the validation texts. Our model makes label predictions for each token, we need to convert this into a list of word indices for each label. Note that the tokens and words are not the same. A single word may be broken into multiple tokens. Therefore we need to first create a map to change token indices to word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:17:45.189106Z",
     "iopub.status.busy": "2022-02-17T15:17:45.188621Z",
     "iopub.status.idle": "2022-02-17T15:20:51.217045Z",
     "shell.execute_reply": "2022-02-17T15:20:51.216282Z",
     "shell.execute_reply.started": "2022-02-17T15:17:45.189071Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 14:53:15.780663: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2172] Converted 2403/6645 nodes to float16 precision using 124 cast(s) to float16 (excluding Const and Variable casts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 - 78s - 78s/epoch - 794ms/step\n",
      "OOF predictions shape: (1560, 1024, 15)\n"
     ]
    }
   ],
   "source": [
    "p = model.predict([train_tokens[valid_idx,], train_attention[valid_idx,]], \n",
    "                  batch_size=16, verbose=2)\n",
    "print('OOF predictions shape:',p.shape)\n",
    "oof_preds = np.argmax(p,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:20:51.219258Z",
     "iopub.status.busy": "2022-02-17T15:20:51.218746Z",
     "iopub.status.idle": "2022-02-17T15:20:51.225043Z",
     "shell.execute_reply": "2022-02-17T15:20:51.224391Z",
     "shell.execute_reply.started": "2022-02-17T15:20:51.219216Z"
    }
   },
   "outputs": [],
   "source": [
    "target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n",
    "             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:20:51.226581Z",
     "iopub.status.busy": "2022-02-17T15:20:51.226306Z",
     "iopub.status.idle": "2022-02-17T15:20:51.244044Z",
     "shell.execute_reply": "2022-02-17T15:20:51.243397Z",
     "shell.execute_reply.started": "2022-02-17T15:20:51.226546Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:20:51.246209Z",
     "iopub.status.busy": "2022-02-17T15:20:51.245827Z",
     "iopub.status.idle": "2022-02-17T15:21:13.23392Z",
     "shell.execute_reply": "2022-02-17T15:21:13.233221Z",
     "shell.execute_reply.started": "2022-02-17T15:20:51.246169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DBF7EB6A9E02</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48D3F4243F0F</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48D3F4243F0F</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>229 230 231 232 233 234 235 236 237 238 239 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>743904BAD7E5</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>252 253 254 255 256 257 258 259 260 261 262 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5FD1A8FB7F6C</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id     class                                   predictionstring\n",
       "0  DBF7EB6A9E02      Lead  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...\n",
       "1  48D3F4243F0F      Lead  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...\n",
       "2  48D3F4243F0F  Evidence  229 230 231 232 233 234 235 236 237 238 239 24...\n",
       "3  743904BAD7E5  Evidence  252 253 254 255 256 257 258 259 260 261 262 26...\n",
       "4  5FD1A8FB7F6C  Evidence  139 140 141 142 143 144 145 146 147 148 149 15..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:21:13.235431Z",
     "iopub.status.busy": "2022-02-17T15:21:13.235087Z",
     "iopub.status.idle": "2022-02-17T15:21:13.243621Z",
     "shell.execute_reply": "2022-02-17T15:21:13.24273Z",
     "shell.execute_reply.started": "2022-02-17T15:21:13.235393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following classes are present in oof preds:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Lead', 'Evidence', 'Concluding Statement', 'Claim', 'Position'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The following classes are present in oof preds:')\n",
    "oof['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Validation Metric\n",
    "The following code is from Rob Mulla's excellent notebook [here][2]. Our LongFormer single fold model achieves CV score 0.617! Hooray!\n",
    "\n",
    "[2]: https://www.kaggle.com/robikscube/student-writing-competition-twitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:21:13.245522Z",
     "iopub.status.busy": "2022-02-17T15:21:13.245222Z",
     "iopub.status.idle": "2022-02-17T15:21:13.262278Z",
     "shell.execute_reply": "2022-02-17T15:21:13.261392Z",
     "shell.execute_reply.started": "2022-02-17T15:21:13.245487Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:21:13.263794Z",
     "iopub.status.busy": "2022-02-17T15:21:13.263544Z",
     "iopub.status.idle": "2022-02-17T15:21:13.297471Z",
     "shell.execute_reply": "2022-02-17T15:21:13.296553Z",
     "shell.execute_reply.started": "2022-02-17T15:21:13.263758Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:21:13.299229Z",
     "iopub.status.busy": "2022-02-17T15:21:13.298945Z",
     "iopub.status.idle": "2022-02-17T15:21:13.908612Z",
     "shell.execute_reply": "2022-02-17T15:21:13.907888Z",
     "shell.execute_reply.started": "2022-02-17T15:21:13.29919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead 0.5660138976626659\n",
      "Evidence 0.2504996668887408\n",
      "Concluding Statement 0.39499455930359084\n",
      "Claim 0.033224990971469845\n",
      "Position 0.012690355329949238\n",
      "\n",
      "Overall 0.2514846940312834\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distil roberta base 128, 8 epocs from 1e-4 to 1e-8 Overall 0.5391855568785594\n",
    "# distil roberta base 512, 5 epocs from 1e-4 to 1e-6 Overall 0.542305764914767\n",
    "# distil roberta base 512, 5 epocs adamw 1e-4 decay OverallOverall 0.42874356353168075\n",
    "# tf roberta base 512, 5 epocs 5 epocs from 1e-4 to 1e-6 OverallOverall 0.42874356353168075\n",
    "longformer base 42?????\n",
    "longformer base 2048 allen 31 1 epochs?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Test Data\n",
    "We will now infer the test data and create a submission. Our CV is 0.617, let's see what our LB is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T16:07:01.454285Z",
     "iopub.status.busy": "2022-02-17T16:07:01.453935Z",
     "iopub.status.idle": "2022-02-17T16:07:01.55575Z",
     "shell.execute_reply": "2022-02-17T16:07:01.554372Z",
     "shell.execute_reply.started": "2022-02-17T16:07:01.454174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 test texts.\n"
     ]
    }
   ],
   "source": [
    "# GET TEST TEXT IDS\n",
    "files = os.listdir('test')\n",
    "TEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\n",
    "print('There are',len(TEST_IDS),'test texts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T12:42:16.246395Z",
     "iopub.status.busy": "2021-12-29T12:42:16.246128Z",
     "iopub.status.idle": "2021-12-29T12:42:16.276174Z",
     "shell.execute_reply": "2021-12-29T12:42:16.275544Z",
     "shell.execute_reply.started": "2021-12-29T12:42:16.246358Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONVERT TEST TEXT TO TOKENS\n",
    "test_tokens = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\n",
    "test_attention = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\n",
    "\n",
    "for id_num in range(len(TEST_IDS)):\n",
    "        \n",
    "    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n",
    "    n = TEST_IDS[id_num]\n",
    "    name = f'test/{n}.txt'\n",
    "    txt = open(name, 'r').read()\n",
    "    \n",
    "    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "    test_tokens[id_num,] = tokens['input_ids']\n",
    "    test_attention[id_num,] = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T12:42:16.277573Z",
     "iopub.status.busy": "2021-12-29T12:42:16.277306Z",
     "iopub.status.idle": "2021-12-29T12:42:16.913716Z",
     "shell.execute_reply": "2021-12-29T12:42:16.913035Z",
     "shell.execute_reply.started": "2021-12-29T12:42:16.277536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - 246ms/epoch - 246ms/step\n",
      "Test predictions shape: (5, 1024, 15)\n"
     ]
    }
   ],
   "source": [
    "# INFER TEST TEXTS\n",
    "p = model.predict([test_tokens, test_attention], \n",
    "                  batch_size=16, verbose=2)\n",
    "print('Test predictions shape:',p.shape)\n",
    "test_preds = np.argmax(p,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T12:42:16.915291Z",
     "iopub.status.busy": "2021-12-29T12:42:16.914954Z",
     "iopub.status.idle": "2021-12-29T12:42:16.998374Z",
     "shell.execute_reply": "2021-12-29T12:42:16.997567Z",
     "shell.execute_reply.started": "2021-12-29T12:42:16.915252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Position</td>\n",
       "      <td>41 42 43 44 45 46 47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Position</td>\n",
       "      <td>108 109 110 111 112 113 114 115 116 117 118 119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id     class                                   predictionstring\n",
       "0  0FB0700DAF44      Lead  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...\n",
       "1  0FB0700DAF44  Position                               41 42 43 44 45 46 47\n",
       "2  0FB0700DAF44     Claim    49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\n",
       "3  0FB0700DAF44     Claim  66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 8...\n",
       "4  0FB0700DAF44  Position    108 109 110 111 112 113 114 115 116 117 118 119"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET TEST PREDICIONS\n",
    "sub = get_preds( dataset='test', verbose=False, text_ids=TEST_IDS, preds=test_preds )\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:26:43.402231Z",
     "iopub.status.busy": "2021-12-29T13:26:43.401962Z",
     "iopub.status.idle": "2021-12-29T13:26:43.437077Z",
     "shell.execute_reply": "2021-12-29T13:26:43.436377Z",
     "shell.execute_reply.started": "2021-12-29T13:26:43.4022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Position</td>\n",
       "      <td>41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Position</td>\n",
       "      <td>108 109 110 111 112 113 114 115 116 117 118 119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id     class                                   predictionstring\n",
       "0  0FB0700DAF44      Lead  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...\n",
       "1  0FB0700DAF44  Position  41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 5...\n",
       "2  0FB0700DAF44     Claim  41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 5...\n",
       "3  0FB0700DAF44     Claim  65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 8...\n",
       "4  0FB0700DAF44  Position    108 109 110 111 112 113 114 115 116 117 118 119"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir('test')\n",
    "TEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\n",
    "for id_num in range(len(TEST_IDS)):\n",
    "        \n",
    "    n = TEST_IDS[id_num]\n",
    "    name = f'test/{n}.txt'\n",
    "    txt = open(name, 'r').read()\n",
    "    \n",
    "    txt = txt.replace('?','.')\n",
    "    phrases = txt.split('.')[0:-1]\n",
    "    \n",
    "    phrase_start = [0]\n",
    "    for phrase in phrases:\n",
    "        phrase_len = len(phrase.split())\n",
    "        phrase_start.append(phrase_start[-1]+phrase_len)\n",
    "        \n",
    "    '''print(phrase_start)\n",
    "    print(' '.join(txt.split()[65:84]))'''\n",
    "    predstrings = sub.loc[sub.id==TEST_IDS[id_num]]['predictionstring']\n",
    "    \n",
    "    corr_predstrings=[]\n",
    "    for i, predstring in enumerate(predstrings):\n",
    "        predstart = int(predstring.split()[0])\n",
    "        predend = int(predstring.split()[-1])\n",
    "        \n",
    "        for j in range(len(phrase_start)-1):\n",
    "            if (predstart > phrase_start[j]) & (predstart < phrase_start[j+1]):\n",
    "                predstart = phrase_start[j]\n",
    "            if (predend > phrase_start[j]) & (predend < phrase_start[j+1]):\n",
    "                predend = phrase_start[j+1]\n",
    "            \n",
    "        predstring = ' '.join([str(val) for val in range(predstart,predend)])\n",
    "        corr_predstrings.append(predstring)\n",
    "    \n",
    "    sub.loc[sub.id==TEST_IDS[id_num], 'predictionstring'] = corr_predstrings\n",
    "    \n",
    "    \n",
    "        \n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T12:42:17.013662Z",
     "iopub.status.busy": "2021-12-29T12:42:17.012962Z",
     "iopub.status.idle": "2021-12-29T12:42:17.020465Z",
     "shell.execute_reply": "2021-12-29T12:42:17.019788Z",
     "shell.execute_reply.started": "2021-12-29T12:42:17.013616Z"
    }
   },
   "outputs": [],
   "source": [
    "# WRITE SUBMISSION CSV\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
