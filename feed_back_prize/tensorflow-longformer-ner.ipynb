{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: This kernel provides some experiments/updates starting from a seminal public kernel by [Chris Deotte](https://www.kaggle.com/cdeotte/tensorflow-longformer-ner-cv-0-617)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a TensorFlow starter notebook for Kaggle's \"Feedback Prize - Evaluating Student Writing\" Competition. Currently this notebook uses\n",
    "* backbone LongFormer\n",
    "* Named Entity Recognition (NER) formulation\n",
    "* one fold\n",
    "\n",
    "With simple changes, we can convert this notebook into Question Answer formulation and we can try different backbones. Furthermore this notebook is one fold. It trains with 90% data and validates on 10% data. We can convert this notebook to K-fold or train with 100% data for boost in LB.\n",
    "\n",
    "The transformer model LongFormer is explained [here][1]. It is similar to Roberta but can accept inputs as wide as 4096 tokens! In this notebook we feed the transformer with 1024 wide tokens. HuggingFace user AllenAI uploaded pretrained weights for us [here][2]\n",
    "\n",
    "[1]: https://huggingface.co/docs/transformers/model_doc/longformer\n",
    "[2]: https://huggingface.co/allenai/longformer-base-4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "This notebook can either train a new model or load a previously trained model (made from previous notebook version). Furthermore, this notebook can either create new NER tokens or load existing tokens (made from previous notebook version). In this notebook version, we will load model and load NER tokens. \n",
    "\n",
    "Also this notebook can load huggingface stuff (like tokenizers) from a Kaggle dataset, or download it from internet. (If it downloads from internet, you can then put it in a Kaggle dataset, so next time you can turn internet off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip feedback-prize-2021.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:12.849936Z",
     "iopub.status.busy": "2022-02-17T14:58:12.849415Z",
     "iopub.status.idle": "2022-02-17T14:58:12.857323Z",
     "shell.execute_reply": "2022-02-17T14:58:12.856592Z",
     "shell.execute_reply.started": "2022-02-17T14:58:12.849898Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# DECLARE HOW MANY GPUS YOU WISH TO USE. \n",
    "# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n",
    "\n",
    "# VERSION FOR SAVING MODEL WEIGHTS\n",
    "VER=12\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n",
    "# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\n",
    "LOAD_TOKENS_FROM = None\n",
    "\n",
    "# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n",
    "# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\n",
    "LOAD_MODEL_FROM = None\n",
    "\n",
    "# IF FOLLOWING IS NONE, THEN NOTEBOOK \n",
    "# USES INTERNET AND DOWNLOADS HUGGINGFACE \n",
    "# CONFIG, TOKENIZER, AND MODEL\n",
    "DOWNLOADED_MODEL_PATH = None\n",
    "if DOWNLOADED_MODEL_PATH is None:\n",
    "    DOWNLOADED_MODEL_PATH = 'model'    \n",
    "# MODEL_NAME = 'distilroberta-base'\n",
    "MODEL_NAME = 'longformer-base'\n",
    "# MODEL_NAME = 'allenai/led-large-16384-arxiv'\n",
    "# MODEL_NAME = 'jplu/tf-xlm-roberta-base'\n",
    "# MODEL_NAME = 'allenai/led-base-16384'\n",
    "# MODEL_NAME = 'xlnet-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Submit TensorFlow Without Internet\n",
    "Many people ask me, how do I submit TensorFlow models without internet? With HuggingFace Transformer, it's easy. Just download the following 3 things (1) model weights, (2) tokenizer files, (3) config file, and upload them to a Kaggle dataset. Below shows code how to get the files from HuggingFace for AllenAI's model `longformer-base`. But this same code can download any transformer, like for example `roberta-base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:13.500244Z",
     "iopub.status.busy": "2022-02-17T14:58:13.499755Z",
     "iopub.status.idle": "2022-02-17T14:58:13.505196Z",
     "shell.execute_reply": "2022-02-17T14:58:13.50451Z",
     "shell.execute_reply.started": "2022-02-17T14:58:13.500205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/longformer-base/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'longformer-base'. Make sure that:\n\n- 'longformer-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'longformer-base' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py:546\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/file_utils.py:1402\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m-> 1402\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/file_utils.py:1574\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1573\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mhead(url, headers\u001b[38;5;241m=\u001b[39mheaders, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout)\n\u001b[0;32m-> 1574\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m etag \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Linked-Etag\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/models.py:940\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m--> 940\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/longformer-base/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoConfig, TFAutoModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     os.mkdir('model')\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME) \n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/tokenization_auto.py:463\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 463\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# If we have the tokenizer class from the tokenizer config or the model config we're good!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/auto/configuration_auto.py:527\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03mInstantiate one of the configuration classes of the library from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;124;03m    {'foo': False}\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    529\u001b[0m     config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/configuration_utils.py:570\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(msg)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m):\n\u001b[1;32m    573\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach server at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to download configuration file or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration file is not a valid JSON file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check network or file content here: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'longformer-base'. Make sure that:\n\n- 'longformer-base' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'longformer-base' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if DOWNLOADED_MODEL_PATH == 'model':\n",
    "    from transformers import AutoTokenizer, AutoConfig, TFAutoModel\n",
    "\n",
    "#     os.mkdir('model')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.save_pretrained('model')\n",
    "\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME) \n",
    "    config.save_pretrained('model')\n",
    "\n",
    "    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n",
    "    backbone.save_pretrained('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above saves the files\n",
    "* TOKENIZER FILES - merges.txt, tokenizer_config.json, special_tokens_map.json, tokenizer.json, vocab.json\n",
    "* CONFIG FILE - config.json\n",
    "* MODEL WEIGHT FILE - tf_model.h5\n",
    "\n",
    "Then just upload all these files to a Kaggle dataset, like what I did [here][1]. Then you load them into your notebook like the notebook you are reading. And we can turn internet off!\n",
    "\n",
    "[1]: https://www.kaggle.com/cdeotte/tf-longformer-v12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas matplotlib sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:14.142138Z",
     "iopub.status.busy": "2022-02-17T14:58:14.141445Z",
     "iopub.status.idle": "2022-02-17T14:58:24.695762Z",
     "shell.execute_reply": "2022-02-17T14:58:24.694269Z",
     "shell.execute_reply.started": "2022-02-17T14:58:14.142102Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #disable all tensorflow logging output\n",
    "\n",
    "from transformers import *\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:24.697979Z",
     "iopub.status.busy": "2022-02-17T14:58:24.697731Z",
     "iopub.status.idle": "2022-02-17T14:58:24.711746Z",
     "shell.execute_reply": "2022-02-17T14:58:24.710975Z",
     "shell.execute_reply.started": "2022-02-17T14:58:24.697941Z"
    }
   },
   "outputs": [],
   "source": [
    "# USE MULTIPLE GPUS\n",
    "if os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('single strategy')\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('multiple strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:24.714268Z",
     "iopub.status.busy": "2022-02-17T14:58:24.713693Z",
     "iopub.status.idle": "2022-02-17T14:58:24.723603Z",
     "shell.execute_reply": "2022-02-17T14:58:24.72279Z",
     "shell.execute_reply.started": "2022-02-17T14:58:24.714231Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "print('Mixed precision enabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:24.72642Z",
     "iopub.status.busy": "2022-02-17T14:58:24.726209Z",
     "iopub.status.idle": "2022-02-17T14:58:26.139158Z",
     "shell.execute_reply": "2022-02-17T14:58:26.137693Z",
     "shell.execute_reply.started": "2022-02-17T14:58:24.726394Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "print( train.shape )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:26.140978Z",
     "iopub.status.busy": "2022-02-17T14:58:26.140714Z",
     "iopub.status.idle": "2022-02-17T14:58:28.505315Z",
     "shell.execute_reply": "2022-02-17T14:58:28.504551Z",
     "shell.execute_reply.started": "2022-02-17T14:58:26.140938Z"
    }
   },
   "outputs": [],
   "source": [
    "# WE ASSUME DATAFRAME IS ASCENDING WHICH IT IS\n",
    "assert( np.sum(train.groupby('id')['discourse_start'].diff()<=0)==0 ) # assert showes no error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.506774Z",
     "iopub.status.busy": "2022-02-17T14:58:28.506526Z",
     "iopub.status.idle": "2022-02-17T14:58:28.526768Z",
     "shell.execute_reply": "2022-02-17T14:58:28.526049Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.50674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train labels are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n",
       "       'Counterclaim', 'Rebuttal'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The train labels are:')\n",
    "train.discourse_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.528257Z",
     "iopub.status.busy": "2022-02-17T14:58:28.528012Z",
     "iopub.status.idle": "2022-02-17T14:58:28.546506Z",
     "shell.execute_reply": "2022-02-17T14:58:28.54586Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.528225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15594 train texts.\n"
     ]
    }
   ],
   "source": [
    "IDS = train.id.unique()\n",
    "print('There are',len(IDS),'train texts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Train\n",
    "The following code converts Kaggle's train dataset into a NER token array that we can use to train a NER transformer. I have made it very clear which targets belong to which class. This allows us to very easily convert this code to `Question Answer formulation` if we want. Just change the 14 NER arrays to be 14 arrays of `start position` and `end position` for each of the 7 classes. (You will need to think creatively what to do if a single text has multiple of one class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.547902Z",
     "iopub.status.busy": "2022-02-17T14:58:28.547603Z",
     "iopub.status.idle": "2022-02-17T14:58:28.696916Z",
     "shell.execute_reply": "2022-02-17T14:58:28.696133Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.547865Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "\n",
    "# THE TOKENS AND ATTENTION ARRAYS\n",
    "tokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH)\n",
    "train_lens = []\n",
    "train_tokens = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
    "train_attention = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n",
    "\n",
    "# THE 14 CLASSES FOR NER\n",
    "lead_b = np.zeros((len(IDS),MAX_LEN))\n",
    "lead_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "position_b = np.zeros((len(IDS),MAX_LEN))\n",
    "position_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "evidence_b = np.zeros((len(IDS),MAX_LEN))\n",
    "evidence_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "claim_b = np.zeros((len(IDS),MAX_LEN))\n",
    "claim_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "conclusion_b = np.zeros((len(IDS),MAX_LEN))\n",
    "conclusion_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "counterclaim_b = np.zeros((len(IDS),MAX_LEN))\n",
    "counterclaim_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "rebuttal_b = np.zeros((len(IDS),MAX_LEN))\n",
    "rebuttal_i = np.zeros((len(IDS),MAX_LEN))\n",
    "\n",
    "# HELPER VARIABLES\n",
    "targets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\n",
    "targets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]\n",
    "target_map = {'Lead':0, 'Position':1, 'Evidence':2, 'Claim':3, 'Concluding Statement':4,\n",
    "             'Counterclaim':5, 'Rebuttal':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.698437Z",
     "iopub.status.busy": "2022-02-17T14:58:28.698151Z",
     "iopub.status.idle": "2022-02-17T14:58:28.710313Z",
     "shell.execute_reply": "2022-02-17T14:58:28.70951Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.698395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , 7800 , 7900 , 8000 , 8100 , 8200 , 8300 , 8400 , 8500 , 8600 , 8700 , 8800 , 8900 , 9000 , 9100 , 9200 , 9300 , 9400 , 9500 , 9600 , 9700 , 9800 , 9900 , 10000 , 10100 , 10200 , 10300 , 10400 , 10500 , 10600 , 10700 , 10800 , 10900 , 11000 , 11100 , 11200 , 11300 , 11400 , 11500 , 11600 , 11700 , 11800 , 11900 , 12000 , 12100 , 12200 , 12300 , 12400 , 12500 , 12600 , 12700 , 12800 , 12900 , 13000 , 13100 , 13200 , 13300 , 13400 , 13500 , 13600 , 13700 , 13800 , 13900 , 14000 , 14100 , 14200 , 14300 , 14400 , 14500 , 14600 , 14700 , 14800 , 14900 , 15000 , 15100 , 15200 , 15300 , 15400 , 15500 , "
     ]
    }
   ],
   "source": [
    "# FOR LOOP THROUGH EACH TRAIN TEXT\n",
    "for id_num in range(len(IDS)):\n",
    "    if LOAD_TOKENS_FROM: break\n",
    "    if id_num%100==0: print(id_num,', ',end='')\n",
    "        \n",
    "    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n",
    "    n = IDS[id_num]\n",
    "    name = f'train/{n}.txt'\n",
    "    txt = open(name, 'r').read()\n",
    "    train_lens.append( len(txt.split()))\n",
    "    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "    train_tokens[id_num,] = tokens['input_ids'] # keys from words in text\n",
    "    train_attention[id_num,] = tokens['attention_mask'] # 1 for each word in text, 0 for padding\n",
    "    \n",
    "    # FIND TARGETS IN TEXT AND SAVE IN TARGET ARRAYS\n",
    "    offsets = tokens['offset_mapping'] # position (start, end) of each word, including special characters such as \\n\n",
    "    offset_index = 0\n",
    "    df = train.loc[train.id==n] # extract train data related to actual text\n",
    "    for index,row in df.iterrows():\n",
    "        a = row.discourse_start\n",
    "        b = row.discourse_end\n",
    "        \n",
    "        if offset_index>len(offsets)-1:\n",
    "            break\n",
    "            \n",
    "        c = offsets[offset_index][0]\n",
    "        d = offsets[offset_index][1]\n",
    "        \n",
    "        beginning = True\n",
    "        \n",
    "        while b>c:\n",
    "            if (c>=a)&(b>=d): # word is in the target\n",
    "                k = target_map[row.discourse_type]\n",
    "                if beginning:\n",
    "                    targets_b[k][id_num][offset_index] = 1 \n",
    "                    beginning = False\n",
    "                else:\n",
    "                    targets_i[k][id_num][offset_index] = 1 \n",
    "            offset_index += 1\n",
    "            if offset_index>len(offsets)-1:\n",
    "                break\n",
    "            c = offsets[offset_index][0]\n",
    "            d = offsets[offset_index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.713498Z",
     "iopub.status.busy": "2022-02-17T14:58:28.71316Z",
     "iopub.status.idle": "2022-02-17T14:58:28.721296Z",
     "shell.execute_reply": "2022-02-17T14:58:28.720612Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.71347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if LOAD_TOKENS_FROM is None:\n",
    "    plt.hist(train_lens,bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "From the histogram of train token counts above, we see that using a transformer width of 1024 is a good comprise of capturing most of the data's signal but not having too large a model. We could probably explore other widths between 512 and 1024 also. Or we could use widths of size 512 or smaller and use a stride which breaks a single text into multiple chunks (with possible overlap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.722952Z",
     "iopub.status.busy": "2022-02-17T14:58:28.722707Z",
     "iopub.status.idle": "2022-02-17T14:58:28.730592Z",
     "shell.execute_reply": "2022-02-17T14:58:28.729882Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.722918Z"
    }
   },
   "outputs": [],
   "source": [
    "if LOAD_TOKENS_FROM is None:\n",
    "    targets = np.zeros((len(IDS),MAX_LEN,15), dtype='int32')\n",
    "    for k in range(7):\n",
    "        targets[:,:,2*k] = targets_b[k]\n",
    "        targets[:,:,2*k+1] = targets_i[k]\n",
    "    targets[:,:,14] = 1-np.max(targets,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:28.733221Z",
     "iopub.status.busy": "2022-02-17T14:58:28.73303Z",
     "iopub.status.idle": "2022-02-17T14:58:36.824137Z",
     "shell.execute_reply": "2022-02-17T14:58:36.8234Z",
     "shell.execute_reply.started": "2022-02-17T14:58:28.733196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved NER tokens\n"
     ]
    }
   ],
   "source": [
    "if LOAD_TOKENS_FROM is None:\n",
    "    np.save(f'targets_{MAX_LEN}', targets)\n",
    "    np.save(f'tokens_{MAX_LEN}', train_tokens)\n",
    "    np.save(f'attention_{MAX_LEN}', train_attention)\n",
    "    print('Saved NER tokens')\n",
    "else:\n",
    "    targets = np.load(f'{LOAD_TOKENS_FROM}/targets_{MAX_LEN}.npy')\n",
    "    train_tokens = np.load(f'{LOAD_TOKENS_FROM}/tokens_{MAX_LEN}.npy')\n",
    "    train_attention = np.load(f'{LOAD_TOKENS_FROM}/attention_{MAX_LEN}.npy')\n",
    "    print('Loaded NER tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model\n",
    "We will use LongFormer backbone and add our own NER head using one hidden layer of size 256 and one final layer with softmax. We use 15 classes because we have a `B` class and `I` class for each of 7 labels. And we have an additional class (called `O` class) for tokens that do not belong to one of the 14 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "#     def __init__(self, d_model, warmup_steps=500):\n",
    "#         super(CustomSchedule, self).__init__()\n",
    "\n",
    "#         self.d_model = d_model\n",
    "#         self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "#         self.warmup_steps = warmup_steps\n",
    "\n",
    "#     def __call__(self, step):\n",
    "#         arg1 = tf.math.rsqrt(step)\n",
    "#         arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "#         return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "# learning_rate = CustomSchedule(20000)\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(\n",
    "#     learning_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-8, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundaries = [500, 3000]\n",
    "# values = [1e-4, 1e-5, 1e-6]\n",
    "# learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "#     boundaries, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_addons as tfa\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:36.825689Z",
     "iopub.status.busy": "2022-02-17T14:58:36.825443Z",
     "iopub.status.idle": "2022-02-17T14:58:36.834316Z",
     "shell.execute_reply": "2022-02-17T14:58:36.833138Z",
     "shell.execute_reply.started": "2022-02-17T14:58:36.825654Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n",
    "    attention = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'attention', dtype=tf.int32)\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'/config.json') \n",
    "    backbone = TFAutoModel.from_pretrained(DOWNLOADED_MODEL_PATH+'/tf_model.h5', config=config, trainable=True)\n",
    "    \n",
    "    x = backbone(tokens, attention_mask=attention)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x[0])\n",
    "    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[tokens,attention], outputs=x)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss =tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:58:36.83675Z",
     "iopub.status.busy": "2022-02-17T14:58:36.835892Z",
     "iopub.status.idle": "2022-02-17T14:59:07.544181Z",
     "shell.execute_reply": "2022-02-17T14:59:07.542427Z",
     "shell.execute_reply.started": "2022-02-17T14:58:36.836712Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at model/tf_model.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train or Load Model\n",
    "If you provide a path in variable `LOAD_MODEL_FROM` above, then it will load your previously trained model. Otherwise it will train now. \n",
    "\n",
    "We train 5 epochs of batch size 32 using learning rate `1e-4` for the first four and `1e-5` for the last epoch. I trained my model offline. If you wish to train on Kaggle's GPU, we may need to reduce the batch size. If we reduce the batch size to 8. That is 1/4 original. So we should also reduce the learning rates to `0.25e-4` and `0.25e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:59:07.546602Z",
     "iopub.status.busy": "2022-02-17T14:59:07.546263Z",
     "iopub.status.idle": "2022-02-17T14:59:07.558327Z",
     "shell.execute_reply": "2022-02-17T14:59:07.557543Z",
     "shell.execute_reply.started": "2022-02-17T14:59:07.546564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 14034 , Valid size 1560\n"
     ]
    }
   ],
   "source": [
    "# TRAIN VALID SPLIT 90% 10%\n",
    "np.random.seed(42)\n",
    "train_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\n",
    "valid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\n",
    "np.random.seed(None)\n",
    "print('Train size',len(train_idx),', Valid size',len(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:59:07.560171Z",
     "iopub.status.busy": "2022-02-17T14:59:07.559917Z",
     "iopub.status.idle": "2022-02-17T14:59:07.565073Z",
     "shell.execute_reply": "2022-02-17T14:59:07.564214Z",
     "shell.execute_reply.started": "2022-02-17T14:59:07.560139Z"
    }
   },
   "outputs": [],
   "source": [
    "# # LEARNING RATE SCHEDULE AND MODEL CHECKPOINT\n",
    "# EPOCHS = 5\n",
    "# LRS = [1e-4, 1e-4, 1e-5, 1e-5, 1e-6]\n",
    "# def lrfn(epoch):\n",
    "#     return LRS[epoch]\n",
    "# lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T14:59:07.567096Z",
     "iopub.status.busy": "2022-02-17T14:59:07.56675Z",
     "iopub.status.idle": "2022-02-17T15:17:20.069387Z",
     "shell.execute_reply": "2022-02-17T15:17:20.068226Z",
     "shell.execute_reply.started": "2022-02-17T14:59:07.567061Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_model_4/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_4/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxlm_roberta_model_4/roberta/pooler/dense/kernel:0', 'tfxlm_roberta_model_4/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 16:07:39.539005: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2172] Converted 1339/6535 nodes to float16 precision using 123 cast(s) to float16 (excluding Const and Variable casts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3509/3509 [==============================] - ETA: 0s - loss: 0.5673 - categorical_accuracy: 0.8209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 16:24:19.431731: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2172] Converted 464/1274 nodes to float16 precision using 26 cast(s) to float16 (excluding Const and Variable casts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3509/3509 [==============================] - 1033s 291ms/step - loss: 0.5673 - categorical_accuracy: 0.8209 - val_loss: 0.4592 - val_categorical_accuracy: 0.8524\n",
      "Epoch 2/3\n",
      "3509/3509 [==============================] - 1020s 291ms/step - loss: 0.4451 - categorical_accuracy: 0.8517 - val_loss: 0.4239 - val_categorical_accuracy: 0.8599\n",
      "Epoch 3/3\n",
      "3509/3509 [==============================] - 1020s 291ms/step - loss: 0.4132 - categorical_accuracy: 0.8590 - val_loss: 0.4211 - val_categorical_accuracy: 0.8587\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL\n",
    "if LOAD_MODEL_FROM:\n",
    "    model.load_weights(f'{LOAD_MODEL_FROM}/long_v14.h5')\n",
    "    \n",
    "# OR TRAIN MODEL\n",
    "else:\n",
    "    model.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n",
    "          y = targets[train_idx,],\n",
    "          validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n",
    "                             targets[valid_idx,]),\n",
    "#           callbacks = [lr_callback],\n",
    "          epochs = 3,\n",
    "          batch_size = 4,\n",
    "          verbose = 1)\n",
    "\n",
    "    # SAVE MODEL WEIGHTS\n",
    "    model.save_weights(f'long_v{VER}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:17:39.725762Z",
     "iopub.status.busy": "2022-02-17T15:17:39.725499Z",
     "iopub.status.idle": "2022-02-17T15:17:41.046651Z",
     "shell.execute_reply": "2022-02-17T15:17:41.045901Z",
     "shell.execute_reply.started": "2022-02-17T15:17:39.725733Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.save_weights(f'long_v{VER}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Model - Infer Out of fold (OOF)\n",
    "We will now make predictions on the validation texts. Our model makes label predictions for each token, we need to convert this into a list of word indices for each label. Note that the tokens and words are not the same. A single word may be broken into multiple tokens. Therefore we need to first create a map to change token indices to word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:17:45.189106Z",
     "iopub.status.busy": "2022-02-17T15:17:45.188621Z",
     "iopub.status.idle": "2022-02-17T15:20:51.217045Z",
     "shell.execute_reply": "2022-02-17T15:20:51.216282Z",
     "shell.execute_reply.started": "2022-02-17T15:17:45.189071Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 17:04:25.009697: I tensorflow/core/grappler/optimizers/auto_mixed_precision.cc:2172] Converted 539/1675 nodes to float16 precision using 26 cast(s) to float16 (excluding Const and Variable casts)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 - 25s - 25s/epoch - 256ms/step\n",
      "OOF predictions shape: (1560, 1024, 15)\n"
     ]
    }
   ],
   "source": [
    "p = model.predict([train_tokens[valid_idx,], train_attention[valid_idx,]], \n",
    "                  batch_size=16, verbose=2)\n",
    "print('OOF predictions shape:',p.shape)\n",
    "oof_preds = np.argmax(p,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:20:51.219258Z",
     "iopub.status.busy": "2022-02-17T15:20:51.218746Z",
     "iopub.status.idle": "2022-02-17T15:20:51.225043Z",
     "shell.execute_reply": "2022-02-17T15:20:51.224391Z",
     "shell.execute_reply.started": "2022-02-17T15:20:51.219216Z"
    }
   },
   "outputs": [],
   "source": [
    "target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n",
    "             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:20:51.226581Z",
     "iopub.status.busy": "2022-02-17T15:20:51.226306Z",
     "iopub.status.idle": "2022-02-17T15:20:51.244044Z",
     "shell.execute_reply": "2022-02-17T15:20:51.243397Z",
     "shell.execute_reply.started": "2022-02-17T15:20:51.226546Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_preds(dataset='train', verbose=True, text_ids=IDS[valid_idx], preds=oof_preds):\n",
    "    all_predictions = []\n",
    "\n",
    "    for id_num in range(len(preds)):\n",
    "    \n",
    "        # GET ID\n",
    "        if (id_num%100==0)&(verbose): \n",
    "            print(id_num,', ',end='')\n",
    "        n = text_ids[id_num]\n",
    "    \n",
    "        # GET TOKEN POSITIONS IN CHARS\n",
    "        name = f'{dataset}/{n}.txt'\n",
    "        txt = open(name, 'r').read()\n",
    "        \n",
    "        '''txt = txt.replace(' .', '. ')\n",
    "        txt = txt.replace(' ,', ', ')'''\n",
    "        \n",
    "        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "        off = tokens['offset_mapping']\n",
    "    \n",
    "        # GET WORD POSITIONS IN CHARS\n",
    "        w = []\n",
    "        blank = True\n",
    "        for i in range(len(txt)):\n",
    "            if (txt[i]!=' ')&(txt[i]!='\\n')&(txt[i]!='\\xa0')&(txt[i]!='\\x85')&(blank==True):\n",
    "                w.append(i)\n",
    "                blank=False\n",
    "            elif (txt[i]==' ')|(txt[i]=='\\n')|(txt[i]=='\\xa0')|(txt[i]=='\\x85'):\n",
    "                blank=True\n",
    "        w.append(1e6)\n",
    "            \n",
    "        # MAPPING FROM TOKENS TO WORDS\n",
    "        word_map = -1 * np.ones(MAX_LEN,dtype='int32')\n",
    "        w_i = 0\n",
    "        for i in range(len(off)):\n",
    "            if off[i][1]==0: continue #ignore first blank\n",
    "            while off[i][0]>=w[w_i+1]: w_i += 1\n",
    "            word_map[i] = int(w_i)\n",
    "        \n",
    "        # CONVERT TOKEN PREDICTIONS INTO WORD LABELS\n",
    "        # KEY:\n",
    "        # 0: LEAD_B, 1: LEAD_I\n",
    "        # 2: POSITION_B, 3: POSITION_I\n",
    "        # 4: EVIDENCE_B, 5: EVIDENCE_I\n",
    "        # 6: CLAIM_B, 7: CLAIM_I\n",
    "        # 8: CONCLUSION_B, 9: CONCLUSION_I\n",
    "        # 10: COUNTERCLAIM_B, 11: COUNTERCLAIM_I\n",
    "        # 12: REBUTTAL_B, 13: REBUTTAL_I\n",
    "        # 14: NOTHING i.e. O\n",
    "        pred = preds[id_num,]/2.0\n",
    "    \n",
    "        i = 0\n",
    "        while i<MAX_LEN:\n",
    "            prediction = []\n",
    "            start = pred[i]\n",
    "            if start in [0,1,2,3,4,5,6,7]:\n",
    "                prediction.append(word_map[i])\n",
    "                i += 1\n",
    "                if i>=MAX_LEN: break\n",
    "                while pred[i]==start+0.5:\n",
    "                    if not word_map[i] in prediction:\n",
    "                        prediction.append(word_map[i])\n",
    "                    i += 1\n",
    "                    if i>=MAX_LEN: break\n",
    "            else:\n",
    "                i += 1\n",
    "            prediction = [x for x in prediction if x!=-1]\n",
    "            if len(prediction)>=4:\n",
    "                all_predictions.append( (n, target_map_rev[int(start)], \n",
    "                                ' '.join([str(x) for x in prediction]) ) )\n",
    "                \n",
    "    # MAKE DATAFRAME\n",
    "    df = pd.DataFrame(all_predictions)\n",
    "    df.columns = ['id','class','predictionstring']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:20:51.246209Z",
     "iopub.status.busy": "2022-02-17T15:20:51.245827Z",
     "iopub.status.idle": "2022-02-17T15:21:13.23392Z",
     "shell.execute_reply": "2022-02-17T15:21:13.233221Z",
     "shell.execute_reply.started": "2022-02-17T15:20:51.246169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50B3435E475B</td>\n",
       "      <td>Lead</td>\n",
       "      <td>3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50B3435E475B</td>\n",
       "      <td>Position</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50B3435E475B</td>\n",
       "      <td>Claim</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50B3435E475B</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>88 89 90 91 92 93 94 95 96 97 98 99 100 101 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50B3435E475B</td>\n",
       "      <td>Claim</td>\n",
       "      <td>179 180 181 182 183 184 185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id     class                                   predictionstring\n",
       "0  50B3435E475B      Lead  3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20...\n",
       "1  50B3435E475B  Position             63 64 65 66 67 68 69 70 71 72 73 74 75\n",
       "2  50B3435E475B     Claim                76 77 78 79 80 81 82 83 84 85 86 87\n",
       "3  50B3435E475B  Evidence  88 89 90 91 92 93 94 95 96 97 98 99 100 101 10...\n",
       "4  50B3435E475B     Claim                        179 180 181 182 183 184 185"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof = get_preds( dataset='train', verbose=True, text_ids=IDS[valid_idx] )\n",
    "oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:21:13.235431Z",
     "iopub.status.busy": "2022-02-17T15:21:13.235087Z",
     "iopub.status.idle": "2022-02-17T15:21:13.243621Z",
     "shell.execute_reply": "2022-02-17T15:21:13.24273Z",
     "shell.execute_reply.started": "2022-02-17T15:21:13.235393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following classes are present in oof preds:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Lead', 'Position', 'Claim', 'Evidence', 'Concluding Statement',\n",
       "       'Counterclaim'], dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The following classes are present in oof preds:')\n",
    "oof['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Validation Metric\n",
    "The following code is from Rob Mulla's excellent notebook [here][2]. Our LongFormer single fold model achieves CV score 0.617! Hooray!\n",
    "\n",
    "[2]: https://www.kaggle.com/robikscube/student-writing-competition-twitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:21:13.245522Z",
     "iopub.status.busy": "2022-02-17T15:21:13.245222Z",
     "iopub.status.idle": "2022-02-17T15:21:13.262278Z",
     "shell.execute_reply": "2022-02-17T15:21:13.261392Z",
     "shell.execute_reply.started": "2022-02-17T15:21:13.245487Z"
    }
   },
   "outputs": [],
   "source": [
    "# CODE FROM : Rob Mulla @robikscube\n",
    "# https://www.kaggle.com/robikscube/student-writing-competition-twitch\n",
    "def calc_overlap(row):\n",
    "    \"\"\"\n",
    "    Calculates the overlap between prediction and\n",
    "    ground truth and overlap percentages used for determining\n",
    "    true positives.\n",
    "    \"\"\"\n",
    "    set_pred = set(row.predictionstring_pred.split(' '))\n",
    "    set_gt = set(row.predictionstring_gt.split(' '))\n",
    "    # Length of each and intersection\n",
    "    len_gt = len(set_gt)\n",
    "    len_pred = len(set_pred)\n",
    "    inter = len(set_gt.intersection(set_pred))\n",
    "    overlap_1 = inter / len_gt\n",
    "    overlap_2 = inter/ len_pred\n",
    "    return [overlap_1, overlap_2]\n",
    "\n",
    "\n",
    "def score_feedback_comp(pred_df, gt_df):\n",
    "    \"\"\"\n",
    "    A function that scores for the kaggle\n",
    "        Student Writing Competition\n",
    "        \n",
    "    Uses the steps in the evaluation page here:\n",
    "        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n",
    "    \"\"\"\n",
    "    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df = pred_df[['id','class','predictionstring']] \\\n",
    "        .reset_index(drop=True).copy()\n",
    "    pred_df['pred_id'] = pred_df.index\n",
    "    gt_df['gt_id'] = gt_df.index\n",
    "    # Step 1. all ground truths and predictions for a given class are compared.\n",
    "    joined = pred_df.merge(gt_df,\n",
    "                           left_on=['id','class'],\n",
    "                           right_on=['id','discourse_type'],\n",
    "                           how='outer',\n",
    "                           suffixes=('_pred','_gt')\n",
    "                          )\n",
    "    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n",
    "    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n",
    "\n",
    "    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n",
    "\n",
    "    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n",
    "    # and the overlap between the prediction and the ground truth >= 0.5,\n",
    "    # the prediction is a match and considered a true positive.\n",
    "    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n",
    "    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n",
    "    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n",
    "\n",
    "\n",
    "    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n",
    "    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n",
    "    tp_pred_ids = joined.query('potential_TP') \\\n",
    "        .sort_values('max_overlap', ascending=False) \\\n",
    "        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n",
    "\n",
    "    # 3. Any unmatched ground truths are false negatives\n",
    "    # and any unmatched predictions are false positives.\n",
    "    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n",
    "\n",
    "    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n",
    "    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n",
    "\n",
    "    # Get numbers of each type\n",
    "    TP = len(tp_pred_ids)\n",
    "    FP = len(fp_pred_ids)\n",
    "    FN = len(unmatched_gt_ids)\n",
    "    #calc microf1\n",
    "    my_f1_score = TP / (TP + 0.5*(FP+FN))\n",
    "    return my_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:21:13.263794Z",
     "iopub.status.busy": "2022-02-17T15:21:13.263544Z",
     "iopub.status.idle": "2022-02-17T15:21:13.297471Z",
     "shell.execute_reply": "2022-02-17T15:21:13.296553Z",
     "shell.execute_reply.started": "2022-02-17T15:21:13.263758Z"
    }
   },
   "outputs": [],
   "source": [
    "# VALID DATAFRAME\n",
    "valid = train.loc[train['id'].isin(IDS[valid_idx])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T15:21:13.299229Z",
     "iopub.status.busy": "2022-02-17T15:21:13.298945Z",
     "iopub.status.idle": "2022-02-17T15:21:13.908612Z",
     "shell.execute_reply": "2022-02-17T15:21:13.907888Z",
     "shell.execute_reply.started": "2022-02-17T15:21:13.29919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead 0.7623369256948384\n",
      "Position 0.5678724231816414\n",
      "Claim 0.4113042324532307\n",
      "Evidence 0.28216197547919186\n",
      "Concluding Statement 0.47058823529411764\n",
      "Counterclaim 0.07153965785381027\n",
      "\n",
      "Overall 0.4276339083261384\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "CLASSES = oof['class'].unique()\n",
    "for c in CLASSES:\n",
    "    pred_df = oof.loc[oof['class']==c].copy()\n",
    "    gt_df = valid.loc[valid['discourse_type']==c].copy()\n",
    "    f1 = score_feedback_comp(pred_df, gt_df)\n",
    "    print(c,f1)\n",
    "    f1s.append(f1)\n",
    "print()\n",
    "print('Overall',np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distil roberta base 128, 8 epocs from 1e-4 to 1e-8 Overall 0.5391855568785594\n",
    "# distil roberta base 512, 5 epocs from 1e-4 to 1e-6 Overall 0.542305764914767\n",
    "# distil roberta base 512, 5 epocs adamw 1e-4 decay OverallOverall 0.42874356353168075\n",
    "# tf roberta base 512, 5 epocs 5 epocs from 1e-4 to 1e-6 OverallOverall 0.42874356353168075\n",
    "longformer base 42?????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Test Data\n",
    "We will now infer the test data and create a submission. Our CV is 0.617, let's see what our LB is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T16:07:01.454285Z",
     "iopub.status.busy": "2022-02-17T16:07:01.453935Z",
     "iopub.status.idle": "2022-02-17T16:07:01.55575Z",
     "shell.execute_reply": "2022-02-17T16:07:01.554372Z",
     "shell.execute_reply.started": "2022-02-17T16:07:01.454174Z"
    }
   },
   "outputs": [],
   "source": [
    "# GET TEST TEXT IDS\n",
    "files = os.listdir('../input/feedback-prize-2021/test')\n",
    "TEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\n",
    "print('There are',len(TEST_IDS),'test texts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T12:42:16.246395Z",
     "iopub.status.busy": "2021-12-29T12:42:16.246128Z",
     "iopub.status.idle": "2021-12-29T12:42:16.276174Z",
     "shell.execute_reply": "2021-12-29T12:42:16.275544Z",
     "shell.execute_reply.started": "2021-12-29T12:42:16.246358Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONVERT TEST TEXT TO TOKENS\n",
    "test_tokens = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\n",
    "test_attention = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\n",
    "\n",
    "for id_num in range(len(TEST_IDS)):\n",
    "        \n",
    "    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n",
    "    n = TEST_IDS[id_num]\n",
    "    name = f'../input/feedback-prize-2021/test/{n}.txt'\n",
    "    txt = open(name, 'r').read()\n",
    "    \n",
    "    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                   truncation=True, return_offsets_mapping=True)\n",
    "    test_tokens[id_num,] = tokens['input_ids']\n",
    "    test_attention[id_num,] = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T12:42:16.277573Z",
     "iopub.status.busy": "2021-12-29T12:42:16.277306Z",
     "iopub.status.idle": "2021-12-29T12:42:16.913716Z",
     "shell.execute_reply": "2021-12-29T12:42:16.913035Z",
     "shell.execute_reply.started": "2021-12-29T12:42:16.277536Z"
    }
   },
   "outputs": [],
   "source": [
    "# INFER TEST TEXTS\n",
    "p = model.predict([test_tokens, test_attention], \n",
    "                  batch_size=16, verbose=2)\n",
    "print('Test predictions shape:',p.shape)\n",
    "test_preds = np.argmax(p,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T12:42:16.915291Z",
     "iopub.status.busy": "2021-12-29T12:42:16.914954Z",
     "iopub.status.idle": "2021-12-29T12:42:16.998374Z",
     "shell.execute_reply": "2021-12-29T12:42:16.997567Z",
     "shell.execute_reply.started": "2021-12-29T12:42:16.915252Z"
    }
   },
   "outputs": [],
   "source": [
    "# GET TEST PREDICIONS\n",
    "sub = get_preds( dataset='test', verbose=False, text_ids=TEST_IDS, preds=test_preds )\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T13:26:43.402231Z",
     "iopub.status.busy": "2021-12-29T13:26:43.401962Z",
     "iopub.status.idle": "2021-12-29T13:26:43.437077Z",
     "shell.execute_reply": "2021-12-29T13:26:43.436377Z",
     "shell.execute_reply.started": "2021-12-29T13:26:43.4022Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir('../input/feedback-prize-2021/test')\n",
    "TEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\n",
    "for id_num in range(len(TEST_IDS)):\n",
    "        \n",
    "    n = TEST_IDS[id_num]\n",
    "    name = f'../input/feedback-prize-2021/test/{n}.txt'\n",
    "    txt = open(name, 'r').read()\n",
    "    \n",
    "    txt = txt.replace('?','.')\n",
    "    phrases = txt.split('.')[0:-1]\n",
    "    \n",
    "    phrase_start = [0]\n",
    "    for phrase in phrases:\n",
    "        phrase_len = len(phrase.split())\n",
    "        phrase_start.append(phrase_start[-1]+phrase_len)\n",
    "        \n",
    "    '''print(phrase_start)\n",
    "    print(' '.join(txt.split()[65:84]))'''\n",
    "    predstrings = sub.loc[sub.id==TEST_IDS[id_num]]['predictionstring']\n",
    "    \n",
    "    corr_predstrings=[]\n",
    "    for i, predstring in enumerate(predstrings):\n",
    "        predstart = int(predstring.split()[0])\n",
    "        predend = int(predstring.split()[-1])\n",
    "        \n",
    "        for j in range(len(phrase_start)-1):\n",
    "            if (predstart > phrase_start[j]) & (predstart < phrase_start[j+1]):\n",
    "                predstart = phrase_start[j]\n",
    "            if (predend > phrase_start[j]) & (predend < phrase_start[j+1]):\n",
    "                predend = phrase_start[j+1]\n",
    "            \n",
    "        predstring = ' '.join([str(val) for val in range(predstart,predend)])\n",
    "        corr_predstrings.append(predstring)\n",
    "    \n",
    "    sub.loc[sub.id==TEST_IDS[id_num], 'predictionstring'] = corr_predstrings\n",
    "    \n",
    "    \n",
    "        \n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-29T12:42:17.013662Z",
     "iopub.status.busy": "2021-12-29T12:42:17.012962Z",
     "iopub.status.idle": "2021-12-29T12:42:17.020465Z",
     "shell.execute_reply": "2021-12-29T12:42:17.019788Z",
     "shell.execute_reply.started": "2021-12-29T12:42:17.013616Z"
    }
   },
   "outputs": [],
   "source": [
    "# WRITE SUBMISSION CSV\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
